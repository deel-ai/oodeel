{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started: Maximul Logit Score on MNIST\n",
    "\n",
    "This notebooks aims at introducing the core features of `oodeel`, including :\n",
    "* Instanciation of `OODDataset` to load a dataset from `tensorflow_datasets` catalog and to organize in-distribution and out-of-distribution data.\n",
    "* Preparation of a `tf.data.Dataset` ready for scoring and training.\n",
    "* A simple utils to train neural nets (adapted when in-distribution is not a standard dataset, such as a subset of class from a dataset)\n",
    "* Instanciation of `OODModel` based on an already trained model, that is used for scoring data.\n",
    "* Some evaluation metrics to assess the quality of OOD detection.\n",
    "\n",
    "First, some required imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" \n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from oodeel.methods import MLS, ODIN\n",
    "from oodeel.eval.metrics import bench_metrics\n",
    "from oodeel.datasets import OODDataset\n",
    "from oodeel.models.training_funs import train_convnet_classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that models are saved at *~/.oodeel/saved_models* by default. Change the following cell for a custom path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = os.path.expanduser(\"~/\") + \".oodeel/saved_models\"\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST vs Fashion MNIST\n",
    "\n",
    "* In-distribution data: MNIST \n",
    "* Out-of-distribution data: Fashion MNIST\n",
    "\n",
    "### Load and prepare the datasets\n",
    "\n",
    "This is performed using the class `OODDataset`. First, load the datasets.\n",
    "\n",
    "/!\\ We denote In-Distribution (ID) data with `_in` and Out-Of-Distribution (OOD) data with `_out` to avoid confusion with OOD detection which is the name of the task, and is therefore used to denote core classes such as `OODDataset` and `OODModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oods_in = OODDataset('mnist', split=\"test\")\n",
    "oods_out = OODDataset('fashion_mnist', split=\"test\")\n",
    "oods_train = OODDataset('mnist', split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, prepare the dataset for scoring and/or training using `.prepare` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(*inputs):\n",
    "    x = inputs[0] / 255\n",
    "    return tuple([x] + list(inputs[1:]))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ds_in = oods_in.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn)\n",
    "ds_out = oods_out.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn)\n",
    "ds_train = oods_train.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train or load a model on in-distribution data (MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_mnist = os.path.join(model_path, \"mnist_model.h5\")\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_path_mnist)\n",
    "    \n",
    "except OSError:\n",
    "    train_config = {\n",
    "        \"input_shape\": (28, 28, 1),\n",
    "        \"num_classes\": 10,\n",
    "        \"batch_size\": 128,\n",
    "        \"is_prepared\": True,\n",
    "        \"epochs\": 5,\n",
    "        \"save_dir\": model_path_mnist,\n",
    "        \"validation_data\": ds_in#.get_dataset() #ds_in is actually the test set of MNIST\n",
    "    }\n",
    "\n",
    "    model = train_convnet_classifier(ds_train, **train_config) #ds_train is actually the train set of MNIST\n",
    "    model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Logit Score (MLS)\n",
    "\n",
    "Return an OOD score based on the maximum value of the output logits. Introduced in [Open-Set Recognition: a Good Closed-Set Classifier is All You Need?](http://arxiv.org/abs/2110.06207), ICLR 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "MLS two datasets"
    ]
   },
   "outputs": [],
   "source": [
    "oodmodel = MLS()\n",
    "oodmodel.fit(model)\n",
    "scores_in = oodmodel.score(ds_in)\n",
    "scores_out = oodmodel.score(ds_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the evaluation metrics based on the scores of the test data, and visualize the scores histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = bench_metrics(\n",
    "    (scores_in, scores_out),  \n",
    "    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n",
    "    threshold = -7.5 # visually chosen based on the plot\n",
    "    )\n",
    "\n",
    "def plot_hist(scores_in, scores_out, bins, log=False):\n",
    "    if log:\n",
    "        minim = np.min([np.min(scores_in), np.min(scores_out)])\n",
    "        scores_in_ = scores_in - 2 * minim + np.min(scores_in[np.where(scores_in != minim)])\n",
    "        scores_out_ = scores_out - 2 * minim + np.min(scores_in[np.where(scores_in != minim)])\n",
    "        _, bins = np.histogram(np.concatenate([scores_in_, scores_out_]), bins=30)\n",
    "        logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"score (normalized log axis)\")\n",
    "    else:\n",
    "        logbins=bins\n",
    "        scores_in_ = scores_in \n",
    "        scores_out_ = scores_out\n",
    "        plt.xlabel(\"score\")\n",
    "    plt.hist((scores_out_, scores_in_), bins=logbins, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plot_hist(scores_in, scores_out, 30)\n",
    "metrics = pd.Series(metrics)\n",
    "print(metrics)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Softmax Score (MSS)\n",
    "\n",
    "It is possible to do the same after the softmax activation. Introduced in [A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks](http://arxiv.org/abs/1610.02136), ICLR 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oodmodel = MLS(output_activation=\"softmax\")\n",
    "oodmodel.fit(model)\n",
    "scores_in = oodmodel.score(ds_in)\n",
    "scores_out = oodmodel.score(ds_out)\n",
    "\n",
    "metrics = bench_metrics(\n",
    "    (scores_in, scores_out),  \n",
    "    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n",
    "    threshold = -0.95 # visually chosen based on the plot\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plot_hist(scores_in, scores_out, 30, log=True)\n",
    "metrics = pd.Series(metrics)\n",
    "print(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST (0-4) vs MNIST (5-9)\n",
    "\n",
    "* In-distribution data: MNIST (0-4)\n",
    "* Out-of-distribution data: MNIST (5-9)\n",
    "\n",
    "We can repeat the procedure in an open-set-recognition or semantic OOD setting by considering a subset of MNIST lasses as in-distribution and another subset as out-of-distribution. The datasets are constructed using the `OODDataset` method `.assign_ood_labels_by_class`. First load and split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oods_train = OODDataset('mnist', split=\"train\")\n",
    "oods_test = OODDataset('mnist', split=\"test\")\n",
    "\n",
    "batch_size = 128\n",
    "inc_labels = [0, 1, 2, 3, 4]\n",
    "oods_train, _ = oods_train.assign_ood_labels_by_class(in_labels=inc_labels)\n",
    "oods_in, oods_out = oods_test.assign_ood_labels_by_class(in_labels=inc_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prepare the datasets for scoring and/or training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_fn(*inputs):\n",
    "    x = inputs[0] / 255\n",
    "    return tuple([x] + list(inputs[1:]))\n",
    "\n",
    "ds_train = oods_train.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn, shuffle=True)\n",
    "ds_in = oods_in.prepare(batch_size=batch_size, with_ood_labels=False, preprocess_fn=preprocess_fn)\n",
    "ds_out = oods_out.prepare(batch_size=batch_size, with_ood_labels=False, preprocess_fn=preprocess_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train or load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_mnist_04 = os.path.join(model_path, \"mnist_model_0-4.h5\")\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_path_mnist_04)\n",
    "    \n",
    "except OSError:\n",
    "    train_config = {\n",
    "        \"input_shape\": (28, 28, 1),\n",
    "        \"num_classes\": 10,\n",
    "        \"is_prepared\": True,\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 5,\n",
    "        \"save_dir\": model_path_mnist_04,\n",
    "        \"validation_data\": ds_in #ds_in is actually the test set of MNIST\n",
    "    }\n",
    "\n",
    "    model = train_convnet_classifier(ds_train, **train_config) #ds_train is actually the train set of MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Logit Score (MLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oodmodel = MLS()\n",
    "oodmodel.fit(model)\n",
    "scores_in = oodmodel.score(ds_in)\n",
    "scores_out = oodmodel.score(ds_out)\n",
    "\n",
    "\n",
    "metrics = bench_metrics(\n",
    "    (scores_in, scores_out),  \n",
    "    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n",
    "    threshold = -7.5 # visually chosen based on the plot\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plot_hist(scores_in, scores_out, 30)\n",
    "metrics = pd.Series(metrics)\n",
    "print(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Softmax Score (MSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oodmodel = MLS(output_activation=\"softmax\")\n",
    "oodmodel.fit(model)\n",
    "scores_in = oodmodel.score(ds_in)\n",
    "scores_out = oodmodel.score(ds_out)\n",
    "\n",
    "\n",
    "metrics = bench_metrics(\n",
    "    (scores_in, scores_out),  \n",
    "    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n",
    "    threshold = -0.95 # visually chosen based on the plot\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plot_hist(scores_in, scores_out, 30, log=True)\n",
    "metrics = pd.Series(metrics)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oodeel_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d80a18d61e6ad16acc6e8a5ef082fbd0a65400a097e8fba89bb5761bc6bbc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
