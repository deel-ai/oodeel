{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark of ODIN method\n",
    "\n",
    "This notebook aims at evaluating the **ODIN method** and reproducing the results in the\n",
    "original paper.\n",
    "\n",
    "The authors compared two network architectures on two in-distribution datasets and six\n",
    "out-of-distribution datasets.\n",
    "\n",
    "- **Network architectures**\n",
    "  - DenseNet-BC-100\n",
    "  - Wide-ResNet-28-10\n",
    "- **ID datasets**\n",
    "  - CIFAR-10\n",
    "  - CIFAR-100\n",
    "- **OOD datasets**\n",
    "  - TinyImageNet cropped\n",
    "  - TinyImageNet resized\n",
    "  - LSUN cropped\n",
    "  - LSUN resized\n",
    "  - Uniform noise\n",
    "  - Gaussian noise.\n",
    "\n",
    "Here, we focus on a DenseNet network trained on CIFAR-10. This model is challenged on\n",
    "LSUN and TinyImageNet cropped OOD datasets.\n",
    "\n",
    "**Reference**  \n",
    "_Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks_  \n",
    "Liang, Shiyu and Li, Yixuan and Srikant, R.  \n",
    "International Conference on Learning Representations, 2018  \n",
    "<https://openreview.net/forum?id=H1VGkIxRZ>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load CIFAR-10 dataset and pretrained DenseNet model\n",
    "\n",
    "The CIFAR-10 dataset is loaded and preprocessed (normalized to 0-1). This is our\n",
    "in-distribution dataset.\n",
    "\n",
    "A pretrained DenseNet-121 model is loaded and evaluated on CIFAR-10 test set: 87.4 %\n",
    "accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Disable TensorFlow log messages\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from oodeel.datasets import OODDataset\n",
    "import pandas as pd\n",
    "\n",
    "model_path = os.path.expanduser(\"~/\") + \".oodeel/saved_models\"\n",
    "data_path = os.path.expanduser(\"~/\") + \".oodeel/datasets\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "\n",
    "input_scaling = 1 / 255\n",
    "batch_size = 128\n",
    "\n",
    "oods_in = OODDataset('cifar10', split=\"test\", input_key=\"image\")\n",
    "oods_train = OODDataset('cifar10', split=\"train\", input_key=\"image\")\n",
    "\n",
    "def preprocess_fn(*inputs):\n",
    "    x = inputs[0] / 255\n",
    "    return tuple([x] + list(inputs[1:]))\n",
    "\n",
    "batch_size = 128\n",
    "ds_in = oods_in.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oodeel.models.training_funs import train_keras_app\n",
    "\n",
    "model_path_cifar = os.path.join(model_path, \"cifar10\")\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_path_cifar)\n",
    "    \n",
    "except OSError:\n",
    "    train_config = {\n",
    "        \"input_shape\": (32, 32, 3),\n",
    "        \"num_classes\": 10,\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 200,\n",
    "        \"save_dir\": model_path_cifar,\n",
    "        \"validation_data\": oods_in.get_dataset() #ds_in is actually the test set of MNIST\n",
    "    }\n",
    "\n",
    "    model = train_keras_app(oods_train.get_dataset(), \"resnet18\", **train_config) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load LSUN cropped dataset\n",
    "\n",
    "Second, the LSUN cropped dataset is loaded and preprocessed. This is our\n",
    "out-of-distribution dataset.\n",
    "\n",
    "**Note**: see https://github.com/facebookresearch/odin to download OOD datasets from the\n",
    "original paper. Some OOD datasets have images of size 36x36 pixels: a black frame of 2 pixels surrounds the\n",
    "32x32 image. In this case, the image is cropped to 32x32 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSUN_root = os.path.join(data_path, \"LSUN\")\n",
    "lsun_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    LSUN_root,\n",
    "    image_size=(32, 32),\n",
    "    shuffle=False,\n",
    "    batch_size=None\n",
    ")\n",
    "# ood dataset\n",
    "ds_out = OODDataset(lsun_ds).prepare(batch_size=batch_size, preprocess_fn=preprocess_fn)\n",
    "\n",
    "\n",
    "ood_name = \"LSUN\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OOD detection\n",
    "\n",
    "Here the ODIN method is applied on both ID and OOD images. The OOD score is computed for\n",
    "each image and some metrics are measured to evaluate the performance of the OOD\n",
    "detector.\n",
    "\n",
    "In a nutshell, the ODIN method consists of the three following steps:\n",
    "\n",
    "1. Perturb the input image by applying a gradient descent step in order to increase the\n",
    "   calibrated probability score of the predicted class.\n",
    "2. Compute the calibrated probability score of the perturbed image. This is defined as\n",
    "   the OOD score.\n",
    "3. If the OOD score is below a threshold, the image is considered as OOD.\n",
    "\n",
    "The _calibrated probability_ is the temperature-scaled softmax of the logits where the\n",
    "temperature is a hyper-parameter of the method. The step in the gradient descent\n",
    "perturbation of the image is also a hyper-parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute OOD scores for ODIN method\n",
    "\n",
    "from oodeel.methods import ODIN\n",
    "\n",
    "oodmodel = ODIN(temperature=1000)\n",
    "oodmodel.fit(model)\n",
    "\n",
    "scores_in = oodmodel.score(ds_in)\n",
    "scores_out = oodmodel.score(ds_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display OOD metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from oodeel.eval.metrics import bench_metrics\n",
    "\n",
    "\n",
    "metrics = bench_metrics(\n",
    "    (scores_in, scores_out),\n",
    "    metrics=[\"auroc\", \"fpr95tpr\"]\n",
    ")\n",
    "metrics = pd.Series(metrics, name='cifar-vs-isun')\n",
    "print(metrics)\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.hist((scores_out, scores_in), bins=100, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\n",
    "plt.xlabel(\"score\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS\n",
    "# Compare calibrated softmax outputs from original and perturbed images (single batch)\n",
    "for img in ds_out.take(1):\n",
    "    img_out = img[0]\n",
    "\n",
    "for img in ds_in.take(1):\n",
    "    img_in = img[0]\n",
    "\n",
    "img_perturbed_out = oodmodel._input_perturbation(img_out)\n",
    "img_perturbed_in = oodmodel._input_perturbation(img_in)\n",
    "perturbations_out = img_perturbed_out - img_out\n",
    "perturbations_in = img_perturbed_in - img_in\n",
    "\n",
    "def get_max_argmax(img):\n",
    "    preds = tf.nn.softmax(model.predict(img) / oodmodel.temperature).numpy()\n",
    "    maxx = preds.max(axis=-1)\n",
    "    argmaxx = preds.argmax(axis=-1).squeeze()\n",
    "    return maxx, argmaxx\n",
    "# Outputs for original images\n",
    "\n",
    "max_out, argmax_out = get_max_argmax(img_out)\n",
    "max_in, argmax_in = get_max_argmax(img_in)\n",
    "max_per_out, argmax_per_out = get_max_argmax(img_perturbed_out)\n",
    "max_per_in, argmax_per_in = get_max_argmax(img_perturbed_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(max_out, \"*--\")\n",
    "plt.plot(max_per_out, \"*--\")\n",
    "plt.title(\"Max of calibrated OOD softmax outputs\")\n",
    "plt.legend([\"Original\", \"Perturbed\"])\n",
    "plt.subplot(222)\n",
    "plt.plot(max_in, \"*--\")\n",
    "plt.plot(max_per_in, \"*--\")\n",
    "plt.title(\"Max of calibrated ID softmax outputs\")\n",
    "plt.legend([\"Original\", \"Perturbed\"])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.hist((max_per_out, max_per_in), bins=30, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\n",
    "plt.title(\"max perturbated softmax for OOD and ID data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.hist((max_per_out - max_out, max_per_in - max_in), bins=30, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\n",
    "plt.title(\"difference between max perturbated softmax and max softmax for OOD and ID data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "nb_label_shifts_out = np.where(argmax_out != argmax_per_out)[0].shape[0]\n",
    "nb_label_shifts_in = np.where(argmax_in != argmax_per_in)[0].shape[0]\n",
    "print(\"Number of label shift for OOD data: \", nb_label_shifts_out)\n",
    "print(\"Number of label shift for ID data: \", nb_label_shifts_in)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oodeel_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d80a18d61e6ad16acc6e8a5ef082fbd0a65400a097e8fba89bb5761bc6bbc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
