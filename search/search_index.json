{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Oodeel is a library that performs post-hoc deep OOD detection on already trained neural network image classifiers.   Works for keras models and will soon be available for pytorch models. The philosophy of the library is to favor quality over quantity and to foster easy adoption. As a result, we provide a simple, compact and easily customizable API and carefully integrate and test each proposed baseline into a coherent framework that is designed to enable their use in tensorflow and (soon) pytorch.</p> <p><pre><code>from oodeel.methods import MLS\n\noodmodel = MLS()\noodmodel.fit(model)\nscores = oodmodel.score(ds)\n</code></pre> Disclaimer: It is still very much a work in progress, see issues and development roadmap. Please use the lib carefully !</p>"},{"location":"#table-of-contents","title":"Table of contents","text":"<ul> <li>Tutorials</li> <li>Quick Start</li> <li>What's Included</li> <li>Development roadmap</li> <li>Contributing</li> <li>See Also</li> <li>Acknowledgments</li> <li>Creator</li> <li>License</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>We propose some tutorials to get familiar with the library and its API in the Tutorials section.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Oodeel requires some stuff and several libraries including Numpy. Installation can be done using Pypi:</p> <pre><code>git clone https://github.com/Jingkang50/OpenOOD.git\ncd oodeel\nmake prepare-dev\n</code></pre> <p>Now that oodeel is installed, here are some basic examples of what you can do with the available modules. See also the notebooks directory for more advanced examples.</p>"},{"location":"#for-benchmarking-with-one-dataset-as-in-distribution-and-another-as-out-of-distribution","title":"For benchmarking with one dataset as in-distribution and another as out-of-distribution","text":"<p>Load in-distribution and out-of-distribution datasets.</p> <pre><code>from oodeel.datasets import OODDataset\n\nds_in = OODDataset('mnist', split=\"test\").prepare(batch_size)\nds_out = OODDataset('fashion_mnist', split=\"test\").prepare(batch_size)\n</code></pre>"},{"location":"#for-benchmarking-with-one-dataset-as-in-distribution-and-another-as-out-of-distribution_1","title":"For benchmarking with one dataset as in-distribution and another as out-of-distribution","text":"<p>Load a dataset and split it into an in-distribution dataset and ou-of-distribution dataset depending on its label values (common practice of anomaly detection and open set recognition).</p> <pre><code>from oodeel.datasets import OODDataset\n\nin_labels = [0, 1, 2, 3, 4]\noods_in, oods_out = oods_test.assign_ood_labels_by_class(in_labels=in_labels)\nds_in = oods_in.prepare(batch_size=batch_size)\nds_out = oods_out.prepare(batch_size=batch_size)\n</code></pre>"},{"location":"#run-an-ood-method","title":"Run an OOD method","text":"<p>Load an OOD method and use it on an already trained model</p> <pre><code>from oodeel.methods import MLS\n\noodmodel = MLS()\noodmodel.fit(model)\nscores_in = oodmodel.score(ds_in)\nscores_out = oodmodel.score(ds_in)\n</code></pre> <p>Evaluate the method</p> <pre><code>from oodeel.eval.metrics import bench_metrics\n\nmetrics = bench_metrics(\n    (scores_in, scores_out),\n    metrics = [\"auroc\", \"fpr95tpr\"],\n    )\n</code></pre>"},{"location":"#whats-included","title":"What's Included","text":"<p>The library is based on a class, <code>OODModel</code>, that fits a model and then scores new samples. Some baselines use extra data, so <code>OODmodel</code> can also fit additional data if needed. The library uses <code>OODDataset</code> to properly load data from different sources and prepare it for OOD detection. It can perform OOD-specific operations like adding extra OOD data for tuning with Outlier Exposure or filters according to label values for anomaly detection or open set recognition benchmarks.</p> <p>Currently, oodeel includes the following baselines:</p> Name Link Venue Status MLS Open-Set Recognition: a Good Closed-Set Classifier is All You Need? ICLR 2022 avail MSS A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks ICLR 2017 avail Mahalanobis A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks NeurIPS 2018 in dev Energy Energy-based Out-of-distribution Detection NeurIPS 2020 avail Odin Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks ICLR 2018 avail DKNN Out-of-Distribution Detection with Deep Nearest Neighbors ICML 2022 avail VIM ViM: Out-Of-Distribution with Virtual-logit Matching CVPR 2022 avail <p>Oodeel also includes standard training functions with data augmentation and learning rate scheduler for models from <code>keras.applications</code> in training_funs directory. These functions come in handy for benchmarks like leave-k-classes-out that require retraining models on a subset of dataset classes.</p>"},{"location":"#development-roadmap","title":"Development Roadmap","text":""},{"location":"#roadmap-to-first-release","title":"Roadmap to first release:","text":"<ul> <li> The library works for <code>keras</code> models</li> <li> Unification of tutorial notebooks</li> <li> Validation of all methods for pytorch using <code>TorchOperator</code>, making oodeel compatible with both tensorflow and pytorch models.</li> <li> Integration of <code>TorchDataHandler</code> to alleviate the need of <code>tf.data.Dataset</code> when using pytorch. At this stage, oodeel will no more require any tensorflow components when using pytorch, and vice-versa.</li> <li> Revise docstring and type hinting</li> <li> Set up the doc</li> </ul>"},{"location":"#whats-next","title":"What's next ?","text":"<ul> <li> More baselines !</li> <li> A module for thorough visualizations (result plots and feature space visualizations)</li> <li> Integrate model loading and uploading with hugginface's transformers library for pretraining</li> <li> Extend the library to more diverse tasks like object detection, segmentation, NLP ...</li> <li> Towards OOD Generalization?</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Feel free to propose your ideas or come and contribute with us on the oodeel toolbox! We have a specific document where we describe in a simple way how to make your first pull request: just here.</p>"},{"location":"#see-also","title":"See Also","text":"<p>Other great tools in the field of OOD:</p> <ul> <li>OpenOOD: Benchmarking Generalized Out-of-Distribution Detection</li> <li>ADBench: Official Implement of \"ADBench: Anomaly Detection Benchmark\".</li> <li>PyOD: A Comprehensive and Scalable Python Library for Outlier Detection (Anomaly Detection)</li> <li>Anomalib: An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.</li> </ul> <p>More from the DEEL project:</p> <ul> <li>Xplique a Python library exclusively dedicated to explaining neural networks.</li> <li>deel-lip a Python library for training k-Lipschitz neural networks on TF.</li> <li>Influenciae Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset.</li> <li>deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch.</li> <li>DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose.</li> </ul>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the  DEEL , a research project jointly conducted in France and Quebec.</p>"},{"location":"#creators","title":"Creators","text":"<p>The library was created by Paul Novello to streamline DEEL research on post-hoc deep OOD methods and foster their adoption by DEEL industrial partners. He was soon joined by Yann Pequignot, Yannick Prudent, Corentin Friedrich and Matthieu Le Goff.</p>"},{"location":"#license","title":"License","text":"<p>The package is released under MIT license.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thanks for taking the time to contribute!</p> <p>From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.</p>"},{"location":"CONTRIBUTING/#setup-with-make","title":"Setup with make","text":"<ul> <li>Clone the repo <code>git clone https://github.com/deel-ai/oodeel.git</code>.</li> <li>Go to your freshly downloaded repo <code>cd oodeel</code></li> <li>Create a virtual environment and install the necessary dependencies for development:</li> </ul> <p><code>make prepare-dev &amp;&amp; source oodeel_dev_env/bin/activate</code>.</p> <p>Welcome to the team !</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>To run test <code>make test</code> This command activate your virtual environment and launch the <code>tox</code> command.</p> <p><code>tox</code> on the otherhand will do the following: - run pytest on the tests folder with python 3.6, python 3.7 and python 3.8</p> <p>Note: If you do not have those 3 interpreters the tests would be only performs with your current interpreter - run pylint on the deel-datasets main files, also with python 3.6, python 3.7 and python 3.8 Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons.</p> <p>Please, make sure you run all the tests at least once before opening a pull request.</p> <p>A word toward Pylint for those that don't know it:</p> <p>Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.</p> <p>Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<p>After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly (see Governance policy).</p> <p>Something that will increase the chance that your pull request is accepted:</p> <ul> <li>Write tests and ensure that the existing ones pass.</li> <li>If <code>make test</code> is succesful, you have fair chances to pass the CI workflows (linting and test)</li> <li>Follow the existing coding style and run <code>make check_all</code> to check all files format.</li> <li>Write a good commit message (we follow a lowercase convention).</li> <li>For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.</li> </ul>"},{"location":"api/methods/","title":"OOD methods","text":""},{"location":"api/methods/#oodeel.methods.base.OODModel","title":"<code>OODModel</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Base Class for methods that assign a score to unseen samples.</p> <p>Parameters:</p> Name Type Description Default <code>output_layers_id</code> <code>List[int]</code> <p>list of str or int that identify features to output. If int, the rank of the layer in the layer list If str, the name of the layer. Defaults to [].</p> <code>[-1]</code> <code>output_activation</code> <p>activation function for the last layer. Defaults to None.</p> required <code>flatten</code> <p>Flatten the output features or not. Defaults to True.</p> required <code>batch_size</code> <p>batch_size used to compute the features space projection of input data. Defaults to 256.</p> required Source code in <code>oodeel/methods/base.py</code> <pre><code>class OODModel(ABC):\n\"\"\"Base Class for methods that assign a score to unseen samples.\n\n    Args:\n        output_layers_id: list of str or int that identify features to output.\n            If int, the rank of the layer in the layer list\n            If str, the name of the layer.\n            Defaults to [].\n        output_activation: activation function for the last layer.\n            Defaults to None.\n        flatten: Flatten the output features or not.\n            Defaults to True.\n        batch_size: batch_size used to compute the features space\n            projection of input data.\n            Defaults to 256.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_layers_id: List[int] = [-1],\n        input_layers_id: List[int] = 0,\n    ):\n        self.feature_extractor = None\n        self.output_layers_id = output_layers_id\n        self.input_layers_id = input_layers_id\n\n    @abstractmethod\n    def _score_tensor(self, inputs: ArrayLike):\n\"\"\"Computes an OOD score for input samples \"inputs\".\n        Method to override with child classes.\n\n        Args:\n            inputs: tensor to score\n\n        Raises:\n            NotImplementedError: _description_\n        \"\"\"\n        raise NotImplementedError()\n\n    def fit(\n        self,\n        model: Callable,\n        fit_dataset: Optional[ArrayLike] = None,\n    ):\n\"\"\"Prepare oodmodel for scoring:\n        * Constructs the feature extractor based on the model\n        * Calibrates the oodmodel on ID data \"fit_dataset\" if needed,\n            using self._fit_to_dataset\n\n        Args:\n            model: model to extract the features from\n            fit_dataset: dataset to fit the oodmodel on\n        \"\"\"\n        self.feature_extractor = self._load_feature_extractor(model)\n\n        if fit_dataset is not None:\n            self._fit_to_dataset(fit_dataset)\n\n    def _load_feature_extractor(\n        self,\n        model: Callable,\n    ) -&gt; Callable:\n\"\"\"\n        Loads feature extractor\n\n        Args:\n            model : tf.keras model (for now)\n                keras models saved as pb files e.g. with model.save()\n        \"\"\"\n        if is_from(model, \"keras\"):\n            from ..utils import TFOperator\n            from ..models.keras_feature_extractor import KerasFeatureExtractor\n\n            self.op = TFOperator()\n            self.backend = \"tensorflow\"\n            FeatureExtractor = KerasFeatureExtractor\n\n        elif is_from(model, \"torch\"):\n            from ..utils import TorchOperator\n            from ..models.torch_feature_extractor import TorchFeatureExtractor\n\n            self.op = TorchOperator()\n            self.backend = \"torch\"\n            FeatureExtractor = TorchFeatureExtractor\n\n        else:\n            raise NotImplementedError()\n\n        feature_extractor = FeatureExtractor(\n            model,\n            input_layer_id=self.input_layers_id,\n            output_layers_id=self.output_layers_id,\n        )\n        return feature_extractor\n\n    def _fit_to_dataset(self, fit_dataset: ArrayLike):\n\"\"\"\n        Fits the oodmodel to fit_dataset.\n        To be overrided in child classes (if needed)\n\n        Args:\n            fit_dataset: dataset to fit the oodmodel on\n\n        Raises:\n            NotImplementedError: _description_\n        \"\"\"\n        raise NotImplementedError()\n\n    def calibrate_threshold(\n        self,\n        fit_dataset: ArrayLike,\n        scores: np.ndarray,\n    ):\n\"\"\"\n        Calibrates the model on ID data \"id_dataset\".\n        Placeholder for now\n\n        Args:\n            fit_dataset: dataset to callibrate the threshold on\n            scores: scores of oodmodel\n\n        Raises:\n            NotImplementedError: _description_\n        \"\"\"\n        raise NotImplementedError()\n\n    def score(\n        self,\n        dataset: ArrayLike,\n    ) -&gt; Union[List[np.ndarray], np.ndarray]:\n\"\"\"\n        Computes an OOD score for input samples \"inputs\"\n\n        Args:\n            inputs: Tensors, or list of tensors to score\n\n        Returns:\n            scores or list of scores (depending on the input)\n        \"\"\"\n        assert self.feature_extractor is not None, \"Call .fit() before .score()\"\n\n        # Case 1: dataset is neither a tf.data.Dataset nor a torch.DataLoader\n        if isinstance(dataset, (np.ndarray, tf.Tensor, tuple)):\n            tensor = get_input_from_dataset_elem(dataset)\n            return self._score_tensor(tensor)\n        # Case 2: dataset is a tf.data.Dataset or a torch.DataLoader\n        else:\n            scores = np.array([])\n            assert is_batched(dataset), \"Please input a batched dataset.\"\n            for tensor in dataset:\n                tensor = get_input_from_dataset_elem(tensor)\n                score_batch = self._score_tensor(tensor)\n                scores = np.append(scores, score_batch)\n        return scores\n\n    def isood(self, inputs: ArrayLike, threshold: float) -&gt; np.ndarray:\n\"\"\"\n        Returns whether the input samples \"inputs\" are OOD or not, given a threshold\n\n        Args:\n            inputs: input samples to score\n            threshold: threshold to use for distinguishing between OOD and ID\n\n        Returns:\n            np.array of 0 for ID samples and 1 for OOD samples\n        \"\"\"\n        scores = self.score(inputs)\n        OODness = tf.map_fn(lambda x: 0 if x &lt; threshold else 1, scores)\n        return OODness\n\n    def __call__(self, inputs: ArrayLike, threshold: float) -&gt; np.ndarray:\n\"\"\"\n        Convenience wrapper for isood\n        \"\"\"\n        return self.isood(inputs, threshold)\n</code></pre>"},{"location":"api/methods/#oodeel.methods.base.OODModel.__call__","title":"<code>__call__(inputs, threshold)</code>","text":"<p>Convenience wrapper for isood</p> Source code in <code>oodeel/methods/base.py</code> <pre><code>def __call__(self, inputs: ArrayLike, threshold: float) -&gt; np.ndarray:\n\"\"\"\n    Convenience wrapper for isood\n    \"\"\"\n    return self.isood(inputs, threshold)\n</code></pre>"},{"location":"api/methods/#oodeel.methods.base.OODModel.calibrate_threshold","title":"<code>calibrate_threshold(fit_dataset, scores)</code>","text":"<p>Calibrates the model on ID data \"id_dataset\". Placeholder for now</p> <p>Parameters:</p> Name Type Description Default <code>fit_dataset</code> <code>ArrayLike</code> <p>dataset to callibrate the threshold on</p> required <code>scores</code> <code>np.ndarray</code> <p>scores of oodmodel</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> Source code in <code>oodeel/methods/base.py</code> <pre><code>def calibrate_threshold(\n    self,\n    fit_dataset: ArrayLike,\n    scores: np.ndarray,\n):\n\"\"\"\n    Calibrates the model on ID data \"id_dataset\".\n    Placeholder for now\n\n    Args:\n        fit_dataset: dataset to callibrate the threshold on\n        scores: scores of oodmodel\n\n    Raises:\n        NotImplementedError: _description_\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/methods/#oodeel.methods.base.OODModel.fit","title":"<code>fit(model, fit_dataset=None)</code>","text":"<p>Prepare oodmodel for scoring: * Constructs the feature extractor based on the model * Calibrates the oodmodel on ID data \"fit_dataset\" if needed,     using self._fit_to_dataset</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Callable</code> <p>model to extract the features from</p> required <code>fit_dataset</code> <code>Optional[ArrayLike]</code> <p>dataset to fit the oodmodel on</p> <code>None</code> Source code in <code>oodeel/methods/base.py</code> <pre><code>def fit(\n    self,\n    model: Callable,\n    fit_dataset: Optional[ArrayLike] = None,\n):\n\"\"\"Prepare oodmodel for scoring:\n    * Constructs the feature extractor based on the model\n    * Calibrates the oodmodel on ID data \"fit_dataset\" if needed,\n        using self._fit_to_dataset\n\n    Args:\n        model: model to extract the features from\n        fit_dataset: dataset to fit the oodmodel on\n    \"\"\"\n    self.feature_extractor = self._load_feature_extractor(model)\n\n    if fit_dataset is not None:\n        self._fit_to_dataset(fit_dataset)\n</code></pre>"},{"location":"api/methods/#oodeel.methods.base.OODModel.isood","title":"<code>isood(inputs, threshold)</code>","text":"<p>Returns whether the input samples \"inputs\" are OOD or not, given a threshold</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>ArrayLike</code> <p>input samples to score</p> required <code>threshold</code> <code>float</code> <p>threshold to use for distinguishing between OOD and ID</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.array of 0 for ID samples and 1 for OOD samples</p> Source code in <code>oodeel/methods/base.py</code> <pre><code>def isood(self, inputs: ArrayLike, threshold: float) -&gt; np.ndarray:\n\"\"\"\n    Returns whether the input samples \"inputs\" are OOD or not, given a threshold\n\n    Args:\n        inputs: input samples to score\n        threshold: threshold to use for distinguishing between OOD and ID\n\n    Returns:\n        np.array of 0 for ID samples and 1 for OOD samples\n    \"\"\"\n    scores = self.score(inputs)\n    OODness = tf.map_fn(lambda x: 0 if x &lt; threshold else 1, scores)\n    return OODness\n</code></pre>"},{"location":"api/methods/#oodeel.methods.base.OODModel.score","title":"<code>score(dataset)</code>","text":"<p>Computes an OOD score for input samples \"inputs\"</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Tensors, or list of tensors to score</p> required <p>Returns:</p> Type Description <code>Union[List[np.ndarray], np.ndarray]</code> <p>scores or list of scores (depending on the input)</p> Source code in <code>oodeel/methods/base.py</code> <pre><code>def score(\n    self,\n    dataset: ArrayLike,\n) -&gt; Union[List[np.ndarray], np.ndarray]:\n\"\"\"\n    Computes an OOD score for input samples \"inputs\"\n\n    Args:\n        inputs: Tensors, or list of tensors to score\n\n    Returns:\n        scores or list of scores (depending on the input)\n    \"\"\"\n    assert self.feature_extractor is not None, \"Call .fit() before .score()\"\n\n    # Case 1: dataset is neither a tf.data.Dataset nor a torch.DataLoader\n    if isinstance(dataset, (np.ndarray, tf.Tensor, tuple)):\n        tensor = get_input_from_dataset_elem(dataset)\n        return self._score_tensor(tensor)\n    # Case 2: dataset is a tf.data.Dataset or a torch.DataLoader\n    else:\n        scores = np.array([])\n        assert is_batched(dataset), \"Please input a batched dataset.\"\n        for tensor in dataset:\n            tensor = get_input_from_dataset_elem(tensor)\n            score_batch = self._score_tensor(tensor)\n            scores = np.append(scores, score_batch)\n    return scores\n</code></pre>"},{"location":"api/methods/#oodeel.methods.DKNN","title":"<code>DKNN</code>","text":"<p>         Bases: <code>OODModel</code></p> <p>\"Out-of-Distribution Detection with Deep Nearest Neighbors\" https://arxiv.org/abs/2204.06507 Simplified version adapted to convnet as built in ./models/train/train_mnist.py</p> <pre><code>Args:\n    nearest: number of nearest neighbors to consider.\n        Defaults to 1.\n    output_layers_id: feature space on which to compute nearest neighbors.\n        Defaults to [-2].\n    output_activation: output activation to use.\n        Defaults to None.\n    flatten: Flatten the output features or not.\n        Defaults to True.\n    batch_size: batch_size used to compute the features space\n        projection of input data.\n        Defaults to 256.\n</code></pre> Source code in <code>oodeel/methods/dknn.py</code> <pre><code>class DKNN(OODModel):\n\"\"\"\n    \"Out-of-Distribution Detection with Deep Nearest Neighbors\"\n    https://arxiv.org/abs/2204.06507\n    Simplified version adapted to convnet as built in ./models/train/train_mnist.py\n\n        Args:\n            nearest: number of nearest neighbors to consider.\n                Defaults to 1.\n            output_layers_id: feature space on which to compute nearest neighbors.\n                Defaults to [-2].\n            output_activation: output activation to use.\n                Defaults to None.\n            flatten: Flatten the output features or not.\n                Defaults to True.\n            batch_size: batch_size used to compute the features space\n                projection of input data.\n                Defaults to 256.\n    \"\"\"\n\n    def __init__(\n        self,\n        nearest: int = 1,\n        output_layers_id: List[int] = [-2],\n    ):\n        super().__init__(\n            output_layers_id=output_layers_id,\n        )\n\n        self.index = None\n        self.nearest = nearest\n\n    def _fit_to_dataset(\n        self, fit_dataset: Union[tf.data.Dataset, tf.Tensor, np.ndarray]\n    ):\n\"\"\"\n        Constructs the index from ID data \"fit_dataset\", which will be used for\n        nearest neighbor search.\n\n        Args:\n            fit_dataset: input dataset (ID) to construct the index with.\n        \"\"\"\n        fit_projected = self.feature_extractor.predict(fit_dataset).numpy()\n        norm_fit_projected = self._l2_normalization(fit_projected)\n        self.index = faiss.IndexFlatL2(norm_fit_projected.shape[1])\n        self.index.add(norm_fit_projected)\n\n    def _score_tensor(\n        self, inputs: Union[tf.data.Dataset, tf.Tensor, np.ndarray]\n    ) -&gt; np.ndarray:\n\"\"\"\n        Computes an OOD score for input samples \"inputs\" based on\n        the distance to nearest neighbors in the feature space of self.model\n\n        Args:\n            inputs: input samples to score\n\n        Returns:\n            scores\n        \"\"\"\n\n        input_projected = self.feature_extractor(inputs).numpy()\n        norm_input_projected = self._l2_normalization(input_projected)\n        scores, _ = self.index.search(norm_input_projected, self.nearest)\n        return scores[:, 0]\n\n    def _l2_normalization(self, feat: np.ndarray) -&gt; np.ndarray:\n        return feat / (np.linalg.norm(feat, ord=2, axis=-1, keepdims=True) + 1e-10)\n</code></pre>"},{"location":"api/methods/#oodeel.methods.Energy","title":"<code>Energy</code>","text":"<p>         Bases: <code>OODModel</code></p> <p>Energy Score method for OOD detection. \"Energy-based Out-of-distribution Detection\" https://arxiv.org/abs/2010.03759</p> <p>This method assumes that the model has been trained with cross entropy loss :math:'CE(model(x))' where :math:'model(x)=(l_{c})_{c=1}^{C}' are the logits predicted for input :math: 'x'. The implementation assumes that the logits are retreieved using the output with linear activation.</p> <p>The energy score for input :math:'x' is given by .. math:: -\\log \\sum_{c=0}^C \\exp(l_c)</p> <p>where 'model(x)=(l_{c})_{c=1}^{C}' are the logits predicted by the model on :math:'x'. As always, training data is expected to have lower score than OOD data.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <p>batch_size used to compute the features space projection of input data. Defaults to 256.</p> required Source code in <code>oodeel/methods/energy.py</code> <pre><code>class Energy(OODModel):\nr\"\"\"\n    Energy Score method for OOD detection.\n    \"Energy-based Out-of-distribution Detection\"\n    https://arxiv.org/abs/2010.03759\n\n    This method assumes that the model has been trained with cross entropy loss\n    :math:'CE(model(x))' where :math:'model(x)=(l_{c})_{c=1}^{C}' are the logits\n    predicted for input :math: 'x'.\n    The implementation assumes that the logits are retreieved using the output with\n    linear activation.\n\n    The energy score for input :math:'x' is given by\n    .. math:: -\\log \\sum_{c=0}^C \\exp(l_c)\n\n    where 'model(x)=(l_{c})_{c=1}^{C}' are the logits predicted by the model on\n    :math:'x'.\n    As always, training data is expected to have lower score than OOD data.\n\n\n    Args:\n        batch_size: batch_size used to compute the features space\n            projection of input data.\n            Defaults to 256.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(output_layers_id=[-1], input_layers_id=0)\n\n    def _score_tensor(\n        self, inputs: Union[tf.data.Dataset, tf.Tensor, np.ndarray]\n    ) -&gt; np.ndarray:\n\"\"\"\n        Computes an OOD score for input samples \"inputs\" based on\n        energy, namey :math:'-logsumexp(logits(inputs))'.\n\n        Args:\n            inputs: input samples to score\n\n        Returns:\n            scores\n        \"\"\"\n\n        # compute logits (softmax(logits,axis=1) is the actual softmax\n        # output minimized using binary cross entropy)\n        logits = self.feature_extractor(inputs)\n        scores = -logsumexp(logits, axis=1)\n        return scores\n</code></pre>"},{"location":"api/methods/#oodeel.methods.MLS","title":"<code>MLS</code>","text":"<p>         Bases: <code>OODModel</code></p> <p>Maximum Logit Scores method for OOD detection. \"Open-Set Recognition: a Good Closed-Set Classifier is All You Need?\" https://arxiv.org/abs/2110.06207</p> <p>Parameters:</p> Name Type Description Default <code>output_activation</code> <p>activation function for the last layer. Defaults to \"linear\".</p> <code>'linear'</code> <code>batch_size</code> <p>batch_size used to compute the features space projection of input data. Defaults to 256.</p> required Source code in <code>oodeel/methods/mls.py</code> <pre><code>class MLS(OODModel):\n\"\"\"\n    Maximum Logit Scores method for OOD detection.\n    \"Open-Set Recognition: a Good Closed-Set Classifier is All You Need?\"\n    https://arxiv.org/abs/2110.06207\n\n\n    Args:\n        output_activation: activation function for the last layer.\n            Defaults to \"linear\".\n        batch_size: batch_size used to compute the features space\n            projection of input data.\n            Defaults to 256.\n    \"\"\"\n\n    def __init__(self, output_activation=\"linear\"):\n        # This line is not necessary but improves readability\n        super().__init__(output_layers_id=[-1], input_layers_id=0)\n        self.output_activation = output_activation\n\n    def _score_tensor(\n        self, inputs: Union[tf.data.Dataset, tf.Tensor, np.ndarray]\n    ) -&gt; np.ndarray:\n\"\"\"\n        Computes an OOD score for input samples \"inputs\" based on\n        the distance to nearest neighbors in the feature space of self.model\n\n        Args:\n            inputs: input samples to score\n\n        Returns:\n            scores\n        \"\"\"\n\n        pred = self.feature_extractor(inputs)\n        if hasattr(tf.keras.activations, self.output_activation):\n            activation = getattr(tf.keras.activations, self.output_activation)\n        pred = activation(pred)\n        scores = -np.max(pred, axis=1)\n        return scores\n</code></pre>"},{"location":"api/methods/#oodeel.methods.ODIN","title":"<code>ODIN</code>","text":"<p>         Bases: <code>OODModel</code></p> <p>\"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks\" http://arxiv.org/abs/1706.02690</p>"},{"location":"api/methods/#oodeel.methods.ODIN--parameters","title":"Parameters","text":"float, optional <p>Temperature parameter, by default 1000</p> float, optional <p>Perturbation noise, by default 0.014</p> int, optional <p>Batch size for score and perturbation computations, by default 256</p> Source code in <code>oodeel/methods/odin.py</code> <pre><code>class ODIN(OODModel):\n\"\"\"\n    \"Enhancing The Reliability of Out-of-distribution Image Detection\n    in Neural Networks\"\n    http://arxiv.org/abs/1706.02690\n\n    Parameters\n    ----------\n    temperature : float, optional\n        Temperature parameter, by default 1000\n    noise : float, optional\n        Perturbation noise, by default 0.014\n    batch_size : int, optional\n        Batch size for score and perturbation computations, by default 256\n    \"\"\"\n\n    def __init__(self, temperature: float = 1000, noise: float = 0.014):\n        self.temperature = temperature\n        super().__init__(output_layers_id=[-1], input_layers_id=0)\n        self.noise = noise\n        self._loss_func = tf.keras.losses.SparseCategoricalCrossentropy(\n            from_logits=True, reduction=\"sum\"\n        )\n\n    def _score_tensor(\n        self, inputs: Union[tf.data.Dataset, tf.Tensor, np.ndarray]\n    ) -&gt; np.ndarray:\n\"\"\"\n        Computes an OOD score for input samples \"inputs\" based on\n        the distance to nearest neighbors in the feature space of self.model\n\n        Args:\n            inputs: input samples to score\n\n        Returns:\n            scores\n        \"\"\"\n        tensor = get_input_from_dataset_elem(inputs)\n        x = self._input_perturbation(tensor)\n        logits = self.feature_extractor.model(x, training=False) / self.temperature\n        preds = self.op.softmax(logits)\n        scores = -self.op.max(preds, axis=1)\n        return scores\n\n    @tf.function\n    def _input_perturbation(self, inputs):\n        preds = self.feature_extractor.model(inputs, training=False)\n        outputs = self.op.argmax(preds, axis=1)\n        gradients = self.op.gradient(self.temperature_loss, inputs, outputs)\n        inputs_p = inputs - self.noise * self.op.sign(gradients)\n        return inputs_p\n\n    def temperature_loss(self, inputs, labels):\n        preds = self.feature_extractor.model(inputs, training=False) / self.temperature\n        loss = self._loss_func(labels, preds)\n        return loss\n</code></pre>"},{"location":"api/methods/#oodeel.methods.VIM","title":"<code>VIM</code>","text":"<p>         Bases: <code>OODModel</code></p> <p>Compute the Virtual Matching Logit (VIM) score. https://arxiv.org/abs/2203.10807</p> <p>This score combines the energy score with a PCA residual score.</p> <p>The energy score is the logarithm of the sum of exponential of logits. The PCA residual score is based on the projection on residual dimensions for principal component analysis.     Residual dimensions are the eigenvectors corresponding to the least eignevalues     (least variance).     Intuitively, this score method assumes that feature representations of ID data     occupy a low dimensional affine subspace :math:'P+c' of the feature space.     Specifically, the projection of ID data translated by :math:'-c' on the     orthognoal complement :math:'P^\\perp' is expected to have small norm.     It allows to detect points whose feature representation lie far from the     identified affine subspace, namely those points :math:'x' such that the     projection on :math:'P^\\perp' of :math:'x-c' has large norm.</p> <p>Parameters:</p> Name Type Description Default <code>princ_dims</code> <code>Union[int, float]</code> <p>number of principal dimensions of in distribution features to</p> <code>None</code> <code>pca_origin</code> <code>str</code> <p>either \"center\" for using the mean of the data in feature space, or</p> <code>'center'</code> <code>output_layers_id</code> <code>List[int]</code> <p>features to use for Residual and Energy score. Defaults to [-2,-1] (-2 the features for PCA residual, -1 the logits with output_activation=\"linear\" for Energy).</p> <code>[-2, -1]</code> <code>output_activation</code> <p>output activation to use. Defaults to \"linear\".</p> required <code>batch_size</code> <p>batch_size used to compute the features space projection of input data. Defaults to 256.</p> required Source code in <code>oodeel/methods/vim.py</code> <pre><code>class VIM(OODModel):\n\"\"\"\n    Compute the Virtual Matching Logit (VIM) score.\n    https://arxiv.org/abs/2203.10807\n\n    This score combines the energy score with a PCA residual score.\n\n    The energy score is the logarithm of the sum of exponential of logits.\n    The PCA residual score is based on the projection on residual dimensions for\n    principal component analysis.\n        Residual dimensions are the eigenvectors corresponding to the least eignevalues\n        (least variance).\n        Intuitively, this score method assumes that feature representations of ID data\n        occupy a low dimensional affine subspace :math:'P+c' of the feature space.\n        Specifically, the projection of ID data translated by :math:'-c' on the\n        orthognoal complement :math:'P^\\perp' is expected to have small norm.\n        It allows to detect points whose feature representation lie far from the\n        identified affine subspace, namely those points :math:'x' such that the\n        projection on :math:'P^\\perp' of :math:'x-c' has large norm.\n\n    Args:\n        princ_dims: number of principal dimensions of in distribution features to\n        consider.\n            If an int, must be less than the dimension of the feature space.\n            If a float, it must be in [0,1), it represents the ratio of explained\n            variance to consider to determine the number of principal components.\n            If None, the kneedle algorithm is used to determine the number of\n            dimensions.\n            Defaults to None.\n        pca_origin: either \"center\" for using the mean of the data in feature space, or\n        \"pseudo\" for using W^{-1}b where W^{-1} is the pseudo inverse of the final\n        linear layer applied to bias term (as in the VIM paper).\n            Defaults to \"center\".\n        output_layers_id: features to use for Residual and Energy score.\n            Defaults to [-2,-1] (-2 the features for PCA residual, -1 the logits with\n            output_activation=\"linear\" for Energy).\n        output_activation: output activation to use.\n            Defaults to \"linear\".\n        batch_size: batch_size used to compute the features space\n            projection of input data.\n            Defaults to 256.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        princ_dims: Union[int, float] = None,\n        pca_origin: str = \"center\",\n        output_layers_id: List[int] = [-2, -1],\n    ):\n\n        super().__init__(\n            output_layers_id=output_layers_id,\n        )\n        self._princ_dim = princ_dims\n        self.pca_origin = pca_origin\n\n    def _fit_to_dataset(\n        self, fit_dataset: Union[tf.data.Dataset, tf.Tensor, np.ndarray]\n    ):\n\"\"\"\n        Computes principal components of feature representations and store the residual\n        eigenvectors.\n        Computes a scaling factor constant :math:'\\alpha' such that the average scaled\n        residual score (on train) is equal to the average maximum logit score (MLS)\n        score.\n\n        Args:\n            fit_dataset: input dataset (ID) to construct the index with.\n        \"\"\"\n        features_train, logits_train = self.feature_extractor.predict(fit_dataset)\n        self.feature_dim = features_train.shape[1]\n        if self.pca_origin == \"center\":\n            self.center = np.mean(features_train, axis=0)\n        elif self.pca_origin == \"pseudo\":\n            W, b = self.feature_extractor.get_weights(-1)\n            self.center = -np.matmul(pinv(W.T), b)\n        else:\n            raise NotImplementedError(\n                'only \"center\" and \"pseudo\" are available for argument \"pca_origin\"'\n            )\n        ec = EmpiricalCovariance(assume_centered=True)\n\n        ec.fit(features_train - self.center)\n        # compute eigenvalues and eigenvectors of empirical covariance matrix\n        eig_vals, eigen_vectors = eigh(ec.covariance_)\n        # allow to use Kneedle to find res_dim\n        self.eigenvalues = eig_vals\n\n        if self._princ_dim is None:\n            if not _has_kneed:\n                raise _kneed_not_found_err\n            # we use kneedle to look for an elbow point to set the number of principal\n            # dimensions\n            # we apply kneedle to the function cumsum(eigvals) which maps a dimension d\n            # to the variance of the d dimensional (principal) subspace with lowest\n            # variance.\n            # since eigvals is non decreasing, cumsum(eigvals) is always convex and\n            # increasing.\n            self.kneedle = KneeLocator(\n                range(len(eig_vals)),\n                np.cumsum(eig_vals),\n                S=1.0,\n                curve=\"convex\",\n                direction=\"increasing\",\n            )\n            self.res_dim = self.kneedle.elbow\n            assert (\n                0 &lt; self.res_dim and self.res_dim &lt; self.feature_dim\n            ), f\"Found invalid number of residual dimensions ({self.res_dim}) \"\n            self._princ_dim = self.feature_dim - self.res_dim\n            print(\n                (\n                    f\"Found an elbow point for {self.feature_dim-self.res_dim} principal \"\n                    \"dimensions inside the {self.feature_dim} dimensional feature space.\"\n                )\n            )\n            print(\n                \"You can visualize this elbow by calling the method '.plot_spectrum()' \"\n                \"of this class\"\n            )\n        elif isinstance(self._princ_dim, int):\n            assert self._princ_dim &lt; self.feature_dim, (\n                f\"if 'princ_dims'(={self._princ_dim}) is an int, it must be less than \"\n                \"feature space dimension ={self.feature_dim})\"\n            )\n            self.res_dim = self.feature_dim - self._princ_dim\n            self._princ_dim = self._princ_dim\n        elif isinstance(self._princ_dim, float):\n            assert (\n                0 &lt;= self._princ_dim and self._princ_dim &lt; 1\n            ), f\"if 'princ_dims'(={self._princ_dim}) is a float, it must be in [0,1)\"\n            explained_variance = np.cumsum(np.flip(eig_vals) / np.sum(eig_vals))\n            self._princ_dim = np.where(explained_variance &gt; self._princ_dim)[0][0]\n            self.res_dim = self.feature_dim - self._princ_dim\n\n        self.res = tf.constant(\n            np.ascontiguousarray(eigen_vectors[:, : self.res_dim], np.float32)\n        )\n\n        # compute residual score on training data\n        train_residual_scores = self._compute_residual_score_tensor(features_train)\n        # compute MLS on training data\n        train_mls_scores = np.max(logits_train, axis=-1)\n        # compute scaling factor\n        self.alpha = np.mean(train_mls_scores) / np.mean(train_residual_scores)\n\n    def _compute_residual_score_tensor(self, features: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"\n        Computes the norm of the residual projection in the feature space.\n\n        Args:\n            features: input samples to score\n\n        Returns:\n            scores\n        \"\"\"\n        # TODO Tensor Compatibility: use of TF tensors\n        # to accelerate matrix multiplication!\n        res_coordinates = tf.matmul(features - self.center, self.res)\n        # taking the norm of the coordinates, which amounts to the norm of\n        # the projection since the eigenvectors form an orthornomal basis\n        res_norm = tf.norm(res_coordinates, axis=-1)\n        return res_norm\n\n    def _residual_score_tensor(\n        self, inputs: Union[tf.data.Dataset, tf.Tensor, np.ndarray]\n    ) -&gt; np.ndarray:\n\"\"\"\n        Computes the residual score for input samples \"inputs\".\n\n        Args:\n            inputs: input samples to score\n\n        Returns:\n            scores\n        \"\"\"\n        assert self.feature_extractor is not None, \"Call .fit() before .score()\"\n        # compute predicted features\n\n        features = self.feature_extractor.predict(inputs)[0]\n        res_scores = self._compute_residual_score_tensor(features)\n        return np.array(res_scores)\n\n    def _score_tensor(\n        self, inputs: Union[tf.data.Dataset, tf.Tensor, np.ndarray]\n    ) -&gt; np.ndarray:\n\"\"\"\n        Computes the VIM score for input samples \"inputs\" as the sum of the energy\n        score and a scaled (PCA) residual norm in the feature space.\n\n        Args:\n            inputs: input samples to score\n\n        Returns:\n            scores\n        \"\"\"\n        # compute predicted features\n\n        features, logits = self.feature_extractor(inputs)\n        res_scores = self._compute_residual_score_tensor(features)\n        energy_scores = logsumexp(logits, axis=-1)\n        scores = -self.alpha * res_scores + energy_scores\n        return -np.array(scores)\n\n    def plot_spectrum(self) -&gt; None:\n        if hasattr(self, \"kneedle\"):\n            self.kneedle.plot_knee()\n            plt.ylabel(\"Explained variance\")\n            plt.xlabel(\"Number of principal dimensions\")\n            plt.title(\n                (\n                    f\"Found elbow at dimension {self.kneedle.elbow}\\n \"\n                    f\"{self.feature_dim-self.kneedle.elbow} principal dimensions\"\n                )\n            )\n        else:\n            plt.plot(np.cumsum(self.eigenvalues))\n            plt.axvline(\n                x=self.res_dim,\n                color=\"r\",\n                linestyle=\"--\",\n                label=f\"Number of principal dimensions = {self._princ_dim} \",\n            )\n            plt.legend()\n            plt.ylabel(\"Explained variance\")\n            plt.xlabel(\"Number of principal dimensions\")\n            plt.title(\"Explained variance by number of principal dimensions\")\n</code></pre>"},{"location":"api/metrics/","title":"Metrics","text":""},{"location":"api/metrics/#oodeel.eval.metrics.bench_metrics","title":"<code>bench_metrics(scores, labels=None, in_value=0, out_value=1, metrics=['auroc', 'fpr95tpr'], threshold=None, step=4)</code>","text":"<p>Compute various common metrics from OODmodel scores. Only AUROC for now. Also returns the positive and negative mtrics curve for visualizations.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <p>scores output of oodmodel to evaluate. If a tuple is provided, the first array is considered in-distribution scores, and the second is considered out-of-distribution scores.</p> required <code>labels</code> <code>Optional[np.ndarray]</code> <p>labels denoting oodness. When labels is not None, the following in_values and out_values are not used. Defaults to None.</p> <code>None</code> <code>in_value</code> <code>Optional[int]</code> <p>ood label value for in-distribution data. Defaults to 0.</p> <code>0</code> <code>out_value</code> <code>Optional[int]</code> <p>ood label value for out-of-distribution data. Defaults to 1.</p> <code>1</code> <code>metrics</code> <code>Optional[List[str]]</code> <p>list of metrics to compute. Can pass any metric name from sklearn.metric. Defaults to [\"auroc\", \"fpr95tpr\"].</p> <code>['auroc', 'fpr95tpr']</code> <code>threshold</code> <code>Optional[float]</code> <p>Threshold to use when using threshold-dependent metrics. Defaults to None.</p> <code>None</code> <code>step</code> <code>Optional[int]</code> <p>integration step (wrt percentile). Only used for auroc and fpr95tpr. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionnary of metrics</p> Source code in <code>oodeel/eval/metrics.py</code> <pre><code>def bench_metrics(\n    scores: Union[np.ndarray, Tuple[np.ndarray, np.ndarray]],\n    labels: Optional[np.ndarray] = None,\n    in_value: Optional[int] = 0,\n    out_value: Optional[int] = 1,\n    metrics: Optional[List[str]] = [\"auroc\", \"fpr95tpr\"],\n    threshold: Optional[float] = None,\n    step: Optional[int] = 4,\n) -&gt; dict:\n\"\"\"Compute various common metrics from OODmodel scores.\n    Only AUROC for now. Also returns the\n    positive and negative mtrics curve for visualizations.\n\n    Args:\n        scores (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): scores output of\n            oodmodel to evaluate. If a tuple is provided,\n            the first array is considered in-distribution scores, and the second\n            is considered out-of-distribution scores.\n        labels (Optional[np.ndarray], optional): labels denoting oodness. When labels\n            is not None, the following in_values and out_values are not used.\n            Defaults to None.\n        in_value (Optional[int], optional): ood label value for in-distribution data.\n            Defaults to 0.\n        out_value (Optional[int], optional): ood label value for out-of-distribution\n            data. Defaults to 1.\n        metrics (Optional[List[str]], optional): list of metrics to compute. Can pass\n            any metric name from sklearn.metric. Defaults to [\"auroc\", \"fpr95tpr\"].\n        threshold (Optional[float], optional): Threshold to use when using\n            threshold-dependent metrics. Defaults to None.\n        step (Optional[int], optional): integration step (wrt percentile).\n            Only used for auroc and fpr95tpr. Defaults to 4.\n\n    Returns:\n        dict: Dictionnary of metrics\n    \"\"\"\n    metrics_dict = {}\n\n    if isinstance(scores, np.ndarray):\n        assert labels is not None, (\n            \"Provide labels with scores, or provide a tuple of in-distribution \"\n            \"and out-of-distribution scores arrays\"\n        )\n    elif isinstance(scores, tuple):\n        scores_in, scores_out = scores\n        scores = np.concatenate([scores_in, scores_out])\n        labels = np.concatenate([scores_in * 0 + in_value, scores_out * 0 + out_value])\n\n    fpr, tpr = get_curve(scores, labels, step)\n\n    for metric in metrics:\n        if metric == \"auroc\":\n            auroc = -np.trapz(1.0 - fpr, tpr)\n            metrics_dict[\"auroc\"] = auroc\n\n        elif metric == \"fpr95tpr\":\n            for i, tp in enumerate(tpr):\n                if tp &lt; 0.95:\n                    ind = i\n                    break\n            metrics_dict[\"fpr95tpr\"] = fpr[ind]\n\n        elif metric.__name__ in sklearn.metrics.__all__:\n            if metric.__name__[:3] == \"roc\":\n                metrics_dict[metric.__name__] = metric(labels, scores)\n            else:\n                if threshold is None:\n                    print(\n                        f\"No threshold is specified for metric {metric.__name__}, \"\n                        \"skipping\"\n                    )\n                else:\n                    oodness = [1 if x &gt; threshold else 0 for x in scores]\n                    metrics_dict[metric.__name__] = metric(labels, oodness)\n\n        else:\n            print(f\"Metric {metric.__name__} not implemented, skipping\")\n\n    return metrics_dict\n</code></pre>"},{"location":"api/metrics/#oodeel.eval.metrics.ftpn","title":"<code>ftpn(scores, labels, threshold)</code>","text":"<p>Computes the number of     * true positives,     * false positives,     * true negatives,     * false negatives, for a given threshold</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>np.ndarray</code> <p>scores output of oodmodel to evaluate</p> required <code>labels</code> <code>np.ndarray</code> <p>1 if ood else 0</p> required <code>threshold</code> <code>float</code> <p>threshold to use to consider scores as in-distribution or out-of-distribution</p> required <p>Returns:</p> Type Description <code>Tuple[float]</code> <p>Tuple[float]: The four metrics</p> Source code in <code>oodeel/eval/metrics.py</code> <pre><code>def ftpn(scores: np.ndarray, labels: np.ndarray, threshold: float) -&gt; Tuple[float]:\n\"\"\"Computes the number of\n        * true positives,\n        * false positives,\n        * true negatives,\n        * false negatives,\n    for a given threshold\n\n    Args:\n        scores (np.ndarray): scores output of oodmodel to evaluate\n        labels (np.ndarray): 1 if ood else 0\n        threshold (float): threshold to use to consider scores\n            as in-distribution or out-of-distribution\n\n    Returns:\n        Tuple[float]: The four metrics\n    \"\"\"\n    pos = np.where(scores &gt;= threshold)\n    neg = np.where(scores &lt; threshold)\n    n_pos = len(pos[0])\n    n_neg = len(neg[0])\n\n    tp = np.sum(labels[pos])\n    fp = n_pos - tp\n    fn = np.sum(labels[neg])\n    tn = n_neg - fn\n\n    return fp, tp, fn, tn\n</code></pre>"},{"location":"api/metrics/#oodeel.eval.metrics.get_curve","title":"<code>get_curve(scores, labels, step=4, return_raw=False)</code>","text":"<p>Computes the number of     * true positives,     * false positives,     * true negatives,     * false negatives, for different threshold values. The values are uniformly distributed among the percentiles, with a step = 4 / scores.shape[0]</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>np.ndarray</code> <p>scores output of oodmodel to evaluate</p> required <code>labels</code> <code>np.ndarray</code> <p>1 if ood else 0</p> required <code>step</code> <code>Optional[int]</code> <p>integration step (wrt percentile). Defaults to 4.</p> <code>4</code> <code>return_raw</code> <code>Optional[bool]</code> <p>To return all the curves or only the rate curves. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[Tuple[np.ndarray], Tuple[np.ndarray]], Tuple[np.ndarray]]</code> <p>Union[Tuple[Tuple[np.ndarray], Tuple[np.ndarray]], Tuple[np.ndarray]]: curves</p> Source code in <code>oodeel/eval/metrics.py</code> <pre><code>def get_curve(\n    scores: np.ndarray,\n    labels: np.ndarray,\n    step: Optional[int] = 4,\n    return_raw: Optional[bool] = False,\n) -&gt; Union[Tuple[Tuple[np.ndarray], Tuple[np.ndarray]], Tuple[np.ndarray]]:\n\"\"\"Computes the number of\n        * true positives,\n        * false positives,\n        * true negatives,\n        * false negatives,\n    for different threshold values. The values are uniformly\n    distributed among the percentiles, with a step = 4 / scores.shape[0]\n\n    Args:\n        scores (np.ndarray): scores output of oodmodel to evaluate\n        labels (np.ndarray): 1 if ood else 0\n        step (Optional[int], optional): integration step (wrt percentile).\n            Defaults to 4.\n        return_raw (Optional[bool], optional): To return all the curves\n            or only the rate curves. Defaults to False.\n\n    Returns:\n        Union[Tuple[Tuple[np.ndarray], Tuple[np.ndarray]], Tuple[np.ndarray]]: curves\n    \"\"\"\n    tpc = np.array([])\n    fpc = np.array([])\n    tnc = np.array([])\n    fnc = np.array([])\n    thresholds = np.sort(scores)\n    for i in range(1, len(scores), step):\n        fp, tp, fn, tn = ftpn(scores, labels, thresholds[i])\n        tpc = np.append(tpc, tp)\n        fpc = np.append(fpc, fp)\n        tnc = np.append(tnc, tn)\n        fnc = np.append(fnc, fn)\n\n    fpr = np.concatenate([[1.0], fpc / (fpc + tnc), [0.0]])\n    tpr = np.concatenate([[1.0], tpc / (tpc + fnc), [0.0]])\n\n    if return_raw:\n        return (fpc, tpc, fnc, tnc), (fpr, tpr)\n    else:\n        return fpr, tpr\n</code></pre>"},{"location":"api/ooddataset/","title":"OOD dataset","text":""},{"location":"api/ooddataset/#oodeel.datasets.ooddataset.OODDataset","title":"<code>OODDataset</code>","text":"<p>         Bases: <code>object</code></p> <p>Class for managing loading and processing of datasets that are to be used for OOD detection. The class encapsulates a dataset like object augmented with OOD related information, and then returns a dataset like object that is suited for scoring or training with the .prepare method.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>Union[tf.data.Dataset, tuple, dict, str]</code> <p>The dataset to load. Can be loaded from tensorflow_datasets catalog when the str matches one of the datasets. Defaults to Union[tf.data.Dataset, tuple, dict, str].</p> required <code>from_directory</code> <code>bool</code> <p>If the dataset has to be loaded from directory, when dataset_id is str. Defaults to False.</p> <code>False</code> <code>backend</code> <code>str</code> <p>Whether the dataset is to be used for tensorflow  or pytorch models. Defaults to \"tensorflow\".</p> <code>'tensorflow'</code> <code>split</code> <code>str</code> <p>Split to use ('test' or 'train') When the dataset is loaded from tensorflow_datasets. Defaults to None.</p> <code>None</code> <code>load_kwargs</code> <code>dict</code> <p>Additional loading kwargs when loading from tensorflow_datasets catalog. Defaults to {}.</p> <code>{}</code> Source code in <code>oodeel/datasets/ooddataset.py</code> <pre><code>class OODDataset(object):\n\"\"\"Class for managing loading and processing of datasets that are to be used for\n    OOD detection. The class encapsulates a dataset like object augmented with OOD\n    related information, and then returns a dataset like object that is suited for\n    scoring or training with the .prepare method.\n\n    Args:\n        dataset_id (Union[tf.data.Dataset, tuple, dict, str]): The dataset to load.\n            Can be loaded from tensorflow_datasets catalog when the str matches one of\n            the datasets. Defaults to Union[tf.data.Dataset, tuple, dict, str].\n        from_directory (bool, optional): If the dataset has to be loaded from directory,\n            when dataset_id is str. Defaults to False.\n        backend (str, optional): Whether the dataset is to be used for tensorflow\n             or pytorch models. Defaults to \"tensorflow\".\n        split (str, optional): Split to use ('test' or 'train') When the dataset is\n            loaded from tensorflow_datasets. Defaults to None.\n        load_kwargs (dict, optional): Additional loading kwargs when loading from\n            tensorflow_datasets catalog. Defaults to {}.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_id: Union[tf.data.Dataset, tuple, dict, str],\n        from_directory: bool = False,\n        backend: str = \"tensorflow\",\n        split: str = None,\n        load_kwargs: dict = {},\n    ):\n        self.backend = backend\n\n        # The length of the dataset is kept as attribute to avoid redundant\n        # iterations over self.data\n        self.length = None\n\n        # Set the load parameters for tfds\n        if load_kwargs is None:\n            load_kwargs = {}\n        load_kwargs[\"as_supervised\"] = False\n        load_kwargs[\"split\"] = split\n        self.load_params = load_kwargs\n\n        # Set the channel order depending on the backend\n        if self.backend in [\"torch\", \"pytorch\"]:\n            tf.config.set_visible_devices([], \"GPU\")\n            self.channel_order = \"channels_first\"\n        else:\n            self.channel_order = \"channels_last\"\n\n        # Load the data handler\n        self._data_handler = TFDataHandler()\n\n        # Load the dataset depending on the type of dataset_id\n        if isinstance(dataset_id, tf.data.Dataset):\n            self.data = self._data_handler.load_tf_ds(dataset_id)\n\n        elif isinstance(dataset_id, (np.ndarray, tuple, dict)):\n            self.data = self._data_handler.load_tf_ds_from_numpy(dataset_id)\n\n        elif isinstance(dataset_id, str):\n            if from_directory:\n                assert os.path.exists(dataset_id), f\"Path {dataset_id} does not exist\"\n                print(f\"Loading from directory {dataset_id}\")\n                # TODO\n            else:\n                self.data, infos = self._data_handler.load_tf_ds_from_tfds(\n                    dataset_id, load_kwargs\n                )\n                self.length = infos.splits[split].num_examples\n\n        # Get the length of the elements in the dataset\n        if self.has_ood_label:\n            self.len_elem = dataset_len_elem(self.data) - 1\n        else:\n            self.len_elem = dataset_len_elem(self.data)\n\n        # Get the key of the tensor to feed the model with\n        self.input_key = self._data_handler.get_ds_feature_keys(self.data)[0]\n\n    def __len__(self):\n\"\"\"get the length of the dataset.\n\n        Returns:\n            int: length of the dataset\n        \"\"\"\n        if self.length is None:\n            self.length = dataset_cardinality(self.data)\n        return self.length\n\n    @property\n    def has_ood_label(self):\n\"\"\"Check if the dataset has an out-of-distribution label.\n\n        Returns:\n            bool: True if the dataset has an out-of-distribution label.\n        \"\"\"\n        return self._data_handler.has_key(self.data, \"ood_label\")\n\n    def get_ood_labels(\n        self,\n    ) -&gt; np.ndarray:\n\"\"\"Get ood_labels from self.data if any\n\n        Returns:\n            np.ndarray: array of labels\n        \"\"\"\n        assert self._data_handler.has_key(\n            self.data, \"ood_label\"\n        ), \"The data has no ood_labels\"\n        labels = self._data_handler.get_feature_from_ds(self.data, \"ood_label\")\n        return labels\n\n    def add_out_data(\n        self,\n        out_dataset: Union[OODDataset, tf.data.Dataset],\n        in_value: int = 0,\n        out_value: int = 1,\n        resize: Optional[bool] = False,\n        shape: Optional[Tuple[int]] = None,\n    ) -&gt; OODDataset:\n\"\"\"Concatenate two OODDatasets. Useful for scoring on multiple datasets, or\n        training with added out-of-distribution data.\n\n        Args:\n            out_dataset (Union[OODDataset, tf.data.Dataset]): dataset of\n                out-of-distribution data\n            in_value (int): ood label value for in-distribution data. Defaults to 0\n            out_value (int): ood label value for out-of-distribution data. Defaults to 1\n            resize (Optional[bool], optional):toggles if input tensors of the\n                datasets have to be resized to have the same shape. Defaults to False.\n            shape (Optional[Tuple[int]], optional):shape to use for resizing input\n                tensors. If None, the tensors are resized with the shape of the\n                in_dataset input tensors. Defaults to None.\n\n        Returns:\n            OODDataset: a Dataset object with the concatenated data\n        \"\"\"\n\n        # Creating an OODDataset object from out_dataset if necessary and make sure\n        # the two OODDatasets have compatible parameters\n        if isinstance(out_dataset, OODDataset):\n            out_dataset = out_dataset.data\n        else:\n            out_dataset = OODDataset(out_dataset).data\n\n        # Assign the correct ood_label to self.data, depending on out_as_in\n        self.data = self._data_handler.assign_feature_value(\n            self.data, \"ood_label\", in_value\n        )\n        out_dataset = self._data_handler.assign_feature_value(\n            out_dataset, \"ood_label\", out_value\n        )\n\n        # Merge the two underlying tf.data.Datasets\n        data = self._data_handler.merge(\n            self.data,\n            out_dataset,\n            resize=resize,\n            shape=shape,\n            channel_order=self.channel_order,\n        )\n\n        # Create a new OODDataset from the merged tf.data.Dataset\n        output_ds = OODDataset(\n            dataset_id=data,\n            backend=self.backend,\n        )\n\n        return output_ds\n\n    def assign_ood_labels_by_class(\n        self,\n        in_labels: Optional[Union[np.ndarray, list]] = None,\n        out_labels: Optional[Union[np.ndarray, list]] = None,\n    ) -&gt; Optional[Tuple[OODDataset]]:\n\"\"\"Filter the dataset by assigning ood labels depending on labels\n        value (typically, class id).\n\n        Args:\n            in_labels (Optional[Union[np.ndarray, list]], optional): set of labels\n                to be considered as in-distribution. Defaults to None.\n            ood_labels (Optional[Union[np.ndarray, list]], optional): set of labels\n                to be considered as out-of-distribution. Defaults to None.\n\n        Returns:\n            Optional[Tuple[OODDataset]]: Tuple of in-distribution and\n                out-of-distribution OODDatasets\n        \"\"\"\n        # Make sure the dataset has labels\n        assert (in_labels is not None) or (\n            out_labels is not None\n        ), \"specify labels to filter with\"\n        assert self.len_elem &gt;= 2, \"the dataset has no labels\"\n\n        # Filter the dataset depending on in_labels and out_labels given\n        if (out_labels is not None) and (in_labels is not None):\n            in_data = self._data_handler.filter_by_feature_value(\n                self.data, \"label\", in_labels\n            )\n            out_data = self._data_handler.filter_by_feature_value(\n                self.data, \"label\", out_labels\n            )\n\n        if out_labels is None:\n            in_data = self._data_handler.filter_by_feature_value(\n                self.data, \"label\", in_labels\n            )\n            out_data = self._data_handler.filter_by_feature_value(\n                self.data, \"label\", in_labels, excluded=True\n            )\n\n        elif in_labels is None:\n            in_data = self._data_handler.filter_by_feature_value(\n                self.data, \"label\", out_labels, excluded=True\n            )\n            out_data = self._data_handler.filter_by_feature_value(\n                self.data, \"label\", out_labels\n            )\n\n        # Return the filtered OODDatasets\n        return (\n            OODDataset(in_data, backend=self.backend),\n            OODDataset(out_data, backend=self.backend),\n        )\n\n    def prepare(\n        self,\n        batch_size: int = 128,\n        preprocess_fn: Callable = None,\n        with_ood_labels: bool = False,\n        with_labels: bool = True,\n        shuffle: bool = False,\n        shuffle_buffer_size: int = None,\n        augment_fn: Callable = None,\n    ) -&gt; tf.data.Dataset:\n\"\"\"Prepare self.data for scoring or training\n\n        Args:\n            batch_size (int, optional): Batch_size of the returned dataset like object.\n                Defaults to 128.\n            preprocess_fn (Callable, optional): Preprocessing function to apply to\n                the dataset. Defaults to None.\n            with_ood_labels (bool, optional): To return the dataset with ood_labels\n                or not. Defaults to True.\n            with_labels (bool, optional): To return the dataset with labels or not.\n                Defaults to True.\n            shuffle (bool, optional): To shuffle the returned dataset or not.\n                Defaults to False.\n            shuffle_buffer_size (int, optional): Size of the shuffle buffer. If None,\n                taken as the number of samples in the dataset. Defaults to None.\n            augment_fn (Callable, optional): Augment function to be used (when the\n                returned dataset is to be used for training). Defaults to None.\n\n        Returns:\n            tf.data.Dataset: prepared dataset\n        \"\"\"\n        # Check if the dataset has at least one of label and ood_label\n        assert (\n            with_ood_labels or with_labels\n        ), \"The dataset must have at least one of label and ood_label\"\n\n        # Check if the dataset has ood_labels when asked to return with_ood_labels\n        if with_ood_labels:\n            assert (\n                self.has_ood_label\n            ), \"Please assign ood labels before preparing with ood_labels\"\n\n        dataset_to_prepare = self.data\n\n        # Making the dataset channel first if the backend is pytorch\n        if self.backend in [\"torch\", \"pytorch\"]:\n            dataset_to_prepare = self._data_handler.make_channel_first(\n                dataset_to_prepare\n            )\n\n        # Select the keys to be returned\n        if with_ood_labels and with_labels:\n            keys = [self.input_key, \"label\", \"ood_label\"]\n        elif with_ood_labels and not with_labels:\n            keys = [self.input_key, \"ood_label\"]\n        else:\n            keys = [self.input_key, \"label\"]\n\n        # Transform the dataset from dict to tuple\n        dataset_to_prepare = self._data_handler.dict_to_tuple(dataset_to_prepare, keys)\n\n        # Apply the preprocessing and augmentation functions if necessary\n        if preprocess_fn is not None:\n            dataset_to_prepare = self._data_handler.map_ds(\n                dataset_to_prepare, preprocess_fn\n            )\n\n        if augment_fn is not None:\n            dataset_to_prepare = self._data_handler.map_ds(\n                dataset_to_prepare, augment_fn\n            )\n\n        # Set the shuffle buffer size if necessary\n        if shuffle:\n            shuffle_buffer_size = (\n                len(self) if shuffle_buffer_size is None else shuffle_buffer_size\n            )\n\n        # Prepare the dataset for training or scoring\n        dataset = self._data_handler.prepare_for_training(\n            dataset_to_prepare, batch_size, shuffle_buffer_size\n        )\n\n        return dataset\n</code></pre>"},{"location":"api/ooddataset/#oodeel.datasets.ooddataset.OODDataset.has_ood_label","title":"<code>has_ood_label</code>  <code>property</code>","text":"<p>Check if the dataset has an out-of-distribution label.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the dataset has an out-of-distribution label.</p>"},{"location":"api/ooddataset/#oodeel.datasets.ooddataset.OODDataset.__len__","title":"<code>__len__()</code>","text":"<p>get the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>length of the dataset</p> Source code in <code>oodeel/datasets/ooddataset.py</code> <pre><code>def __len__(self):\n\"\"\"get the length of the dataset.\n\n    Returns:\n        int: length of the dataset\n    \"\"\"\n    if self.length is None:\n        self.length = dataset_cardinality(self.data)\n    return self.length\n</code></pre>"},{"location":"api/ooddataset/#oodeel.datasets.ooddataset.OODDataset.add_out_data","title":"<code>add_out_data(out_dataset, in_value=0, out_value=1, resize=False, shape=None)</code>","text":"<p>Concatenate two OODDatasets. Useful for scoring on multiple datasets, or training with added out-of-distribution data.</p> <p>Parameters:</p> Name Type Description Default <code>out_dataset</code> <code>Union[OODDataset, tf.data.Dataset]</code> <p>dataset of out-of-distribution data</p> required <code>in_value</code> <code>int</code> <p>ood label value for in-distribution data. Defaults to 0</p> <code>0</code> <code>out_value</code> <code>int</code> <p>ood label value for out-of-distribution data. Defaults to 1</p> <code>1</code> <code>resize</code> <code>Optional[bool]</code> <p>toggles if input tensors of the datasets have to be resized to have the same shape. Defaults to False.</p> <code>False</code> <code>shape</code> <code>Optional[Tuple[int]]</code> <p>shape to use for resizing input tensors. If None, the tensors are resized with the shape of the in_dataset input tensors. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>OODDataset</code> <code>OODDataset</code> <p>a Dataset object with the concatenated data</p> Source code in <code>oodeel/datasets/ooddataset.py</code> <pre><code>def add_out_data(\n    self,\n    out_dataset: Union[OODDataset, tf.data.Dataset],\n    in_value: int = 0,\n    out_value: int = 1,\n    resize: Optional[bool] = False,\n    shape: Optional[Tuple[int]] = None,\n) -&gt; OODDataset:\n\"\"\"Concatenate two OODDatasets. Useful for scoring on multiple datasets, or\n    training with added out-of-distribution data.\n\n    Args:\n        out_dataset (Union[OODDataset, tf.data.Dataset]): dataset of\n            out-of-distribution data\n        in_value (int): ood label value for in-distribution data. Defaults to 0\n        out_value (int): ood label value for out-of-distribution data. Defaults to 1\n        resize (Optional[bool], optional):toggles if input tensors of the\n            datasets have to be resized to have the same shape. Defaults to False.\n        shape (Optional[Tuple[int]], optional):shape to use for resizing input\n            tensors. If None, the tensors are resized with the shape of the\n            in_dataset input tensors. Defaults to None.\n\n    Returns:\n        OODDataset: a Dataset object with the concatenated data\n    \"\"\"\n\n    # Creating an OODDataset object from out_dataset if necessary and make sure\n    # the two OODDatasets have compatible parameters\n    if isinstance(out_dataset, OODDataset):\n        out_dataset = out_dataset.data\n    else:\n        out_dataset = OODDataset(out_dataset).data\n\n    # Assign the correct ood_label to self.data, depending on out_as_in\n    self.data = self._data_handler.assign_feature_value(\n        self.data, \"ood_label\", in_value\n    )\n    out_dataset = self._data_handler.assign_feature_value(\n        out_dataset, \"ood_label\", out_value\n    )\n\n    # Merge the two underlying tf.data.Datasets\n    data = self._data_handler.merge(\n        self.data,\n        out_dataset,\n        resize=resize,\n        shape=shape,\n        channel_order=self.channel_order,\n    )\n\n    # Create a new OODDataset from the merged tf.data.Dataset\n    output_ds = OODDataset(\n        dataset_id=data,\n        backend=self.backend,\n    )\n\n    return output_ds\n</code></pre>"},{"location":"api/ooddataset/#oodeel.datasets.ooddataset.OODDataset.assign_ood_labels_by_class","title":"<code>assign_ood_labels_by_class(in_labels=None, out_labels=None)</code>","text":"<p>Filter the dataset by assigning ood labels depending on labels value (typically, class id).</p> <p>Parameters:</p> Name Type Description Default <code>in_labels</code> <code>Optional[Union[np.ndarray, list]]</code> <p>set of labels to be considered as in-distribution. Defaults to None.</p> <code>None</code> <code>ood_labels</code> <code>Optional[Union[np.ndarray, list]]</code> <p>set of labels to be considered as out-of-distribution. Defaults to None.</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[OODDataset]]</code> <p>Optional[Tuple[OODDataset]]: Tuple of in-distribution and out-of-distribution OODDatasets</p> Source code in <code>oodeel/datasets/ooddataset.py</code> <pre><code>def assign_ood_labels_by_class(\n    self,\n    in_labels: Optional[Union[np.ndarray, list]] = None,\n    out_labels: Optional[Union[np.ndarray, list]] = None,\n) -&gt; Optional[Tuple[OODDataset]]:\n\"\"\"Filter the dataset by assigning ood labels depending on labels\n    value (typically, class id).\n\n    Args:\n        in_labels (Optional[Union[np.ndarray, list]], optional): set of labels\n            to be considered as in-distribution. Defaults to None.\n        ood_labels (Optional[Union[np.ndarray, list]], optional): set of labels\n            to be considered as out-of-distribution. Defaults to None.\n\n    Returns:\n        Optional[Tuple[OODDataset]]: Tuple of in-distribution and\n            out-of-distribution OODDatasets\n    \"\"\"\n    # Make sure the dataset has labels\n    assert (in_labels is not None) or (\n        out_labels is not None\n    ), \"specify labels to filter with\"\n    assert self.len_elem &gt;= 2, \"the dataset has no labels\"\n\n    # Filter the dataset depending on in_labels and out_labels given\n    if (out_labels is not None) and (in_labels is not None):\n        in_data = self._data_handler.filter_by_feature_value(\n            self.data, \"label\", in_labels\n        )\n        out_data = self._data_handler.filter_by_feature_value(\n            self.data, \"label\", out_labels\n        )\n\n    if out_labels is None:\n        in_data = self._data_handler.filter_by_feature_value(\n            self.data, \"label\", in_labels\n        )\n        out_data = self._data_handler.filter_by_feature_value(\n            self.data, \"label\", in_labels, excluded=True\n        )\n\n    elif in_labels is None:\n        in_data = self._data_handler.filter_by_feature_value(\n            self.data, \"label\", out_labels, excluded=True\n        )\n        out_data = self._data_handler.filter_by_feature_value(\n            self.data, \"label\", out_labels\n        )\n\n    # Return the filtered OODDatasets\n    return (\n        OODDataset(in_data, backend=self.backend),\n        OODDataset(out_data, backend=self.backend),\n    )\n</code></pre>"},{"location":"api/ooddataset/#oodeel.datasets.ooddataset.OODDataset.get_ood_labels","title":"<code>get_ood_labels()</code>","text":"<p>Get ood_labels from self.data if any</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: array of labels</p> Source code in <code>oodeel/datasets/ooddataset.py</code> <pre><code>def get_ood_labels(\n    self,\n) -&gt; np.ndarray:\n\"\"\"Get ood_labels from self.data if any\n\n    Returns:\n        np.ndarray: array of labels\n    \"\"\"\n    assert self._data_handler.has_key(\n        self.data, \"ood_label\"\n    ), \"The data has no ood_labels\"\n    labels = self._data_handler.get_feature_from_ds(self.data, \"ood_label\")\n    return labels\n</code></pre>"},{"location":"api/ooddataset/#oodeel.datasets.ooddataset.OODDataset.prepare","title":"<code>prepare(batch_size=128, preprocess_fn=None, with_ood_labels=False, with_labels=True, shuffle=False, shuffle_buffer_size=None, augment_fn=None)</code>","text":"<p>Prepare self.data for scoring or training</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch_size of the returned dataset like object. Defaults to 128.</p> <code>128</code> <code>preprocess_fn</code> <code>Callable</code> <p>Preprocessing function to apply to the dataset. Defaults to None.</p> <code>None</code> <code>with_ood_labels</code> <code>bool</code> <p>To return the dataset with ood_labels or not. Defaults to True.</p> <code>False</code> <code>with_labels</code> <code>bool</code> <p>To return the dataset with labels or not. Defaults to True.</p> <code>True</code> <code>shuffle</code> <code>bool</code> <p>To shuffle the returned dataset or not. Defaults to False.</p> <code>False</code> <code>shuffle_buffer_size</code> <code>int</code> <p>Size of the shuffle buffer. If None, taken as the number of samples in the dataset. Defaults to None.</p> <code>None</code> <code>augment_fn</code> <code>Callable</code> <p>Augment function to be used (when the returned dataset is to be used for training). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tf.data.Dataset</code> <p>tf.data.Dataset: prepared dataset</p> Source code in <code>oodeel/datasets/ooddataset.py</code> <pre><code>def prepare(\n    self,\n    batch_size: int = 128,\n    preprocess_fn: Callable = None,\n    with_ood_labels: bool = False,\n    with_labels: bool = True,\n    shuffle: bool = False,\n    shuffle_buffer_size: int = None,\n    augment_fn: Callable = None,\n) -&gt; tf.data.Dataset:\n\"\"\"Prepare self.data for scoring or training\n\n    Args:\n        batch_size (int, optional): Batch_size of the returned dataset like object.\n            Defaults to 128.\n        preprocess_fn (Callable, optional): Preprocessing function to apply to\n            the dataset. Defaults to None.\n        with_ood_labels (bool, optional): To return the dataset with ood_labels\n            or not. Defaults to True.\n        with_labels (bool, optional): To return the dataset with labels or not.\n            Defaults to True.\n        shuffle (bool, optional): To shuffle the returned dataset or not.\n            Defaults to False.\n        shuffle_buffer_size (int, optional): Size of the shuffle buffer. If None,\n            taken as the number of samples in the dataset. Defaults to None.\n        augment_fn (Callable, optional): Augment function to be used (when the\n            returned dataset is to be used for training). Defaults to None.\n\n    Returns:\n        tf.data.Dataset: prepared dataset\n    \"\"\"\n    # Check if the dataset has at least one of label and ood_label\n    assert (\n        with_ood_labels or with_labels\n    ), \"The dataset must have at least one of label and ood_label\"\n\n    # Check if the dataset has ood_labels when asked to return with_ood_labels\n    if with_ood_labels:\n        assert (\n            self.has_ood_label\n        ), \"Please assign ood labels before preparing with ood_labels\"\n\n    dataset_to_prepare = self.data\n\n    # Making the dataset channel first if the backend is pytorch\n    if self.backend in [\"torch\", \"pytorch\"]:\n        dataset_to_prepare = self._data_handler.make_channel_first(\n            dataset_to_prepare\n        )\n\n    # Select the keys to be returned\n    if with_ood_labels and with_labels:\n        keys = [self.input_key, \"label\", \"ood_label\"]\n    elif with_ood_labels and not with_labels:\n        keys = [self.input_key, \"ood_label\"]\n    else:\n        keys = [self.input_key, \"label\"]\n\n    # Transform the dataset from dict to tuple\n    dataset_to_prepare = self._data_handler.dict_to_tuple(dataset_to_prepare, keys)\n\n    # Apply the preprocessing and augmentation functions if necessary\n    if preprocess_fn is not None:\n        dataset_to_prepare = self._data_handler.map_ds(\n            dataset_to_prepare, preprocess_fn\n        )\n\n    if augment_fn is not None:\n        dataset_to_prepare = self._data_handler.map_ds(\n            dataset_to_prepare, augment_fn\n        )\n\n    # Set the shuffle buffer size if necessary\n    if shuffle:\n        shuffle_buffer_size = (\n            len(self) if shuffle_buffer_size is None else shuffle_buffer_size\n        )\n\n    # Prepare the dataset for training or scoring\n    dataset = self._data_handler.prepare_for_training(\n        dataset_to_prepare, batch_size, shuffle_buffer_size\n    )\n\n    return dataset\n</code></pre>"},{"location":"api/training_funs/","title":"Training tools","text":""},{"location":"api/training_funs/#oodeel.models.training_funs.keras_models","title":"<code>keras_models</code>","text":""},{"location":"api/training_funs/#oodeel.models.training_funs.keras_models.train_keras_app","title":"<code>train_keras_app(train_data, model_name, input_shape=None, num_classes=None, is_prepared=False, batch_size=128, epochs=50, loss='sparse_categorical_crossentropy', optimizer='adam', learning_rate=0.001, metrics=['accuracy'], imagenet_pretrained=False, validation_data=None, save_dir=None)</code>","text":"<p>Loads a model from keras.applications. If the dataset is different from imagenet, trains on provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>must be a model from tf.keras.applications</p> required <code>input_shape</code> <code>tuple</code> <p>If None, infered from train_data. Defaults to None.</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>If None, infered from train_data. Defaults to None.</p> <code>None</code> <code>is_prepared</code> <code>bool</code> <p>If train_data is a pipeline already prepared for training (with batch, shufle, cache etc...). Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Defaults to 128.</p> <code>128</code> <code>epochs</code> <code>int</code> <p>Defaults to 50.</p> <code>50</code> <code>loss</code> <code>str</code> <p>Defaults to \"sparse_categorical_crossentropy\".</p> <code>'sparse_categorical_crossentropy'</code> <code>optimizer</code> <code>str</code> <p>Defaults to \"adam\".</p> <code>'adam'</code> <code>learning_rate</code> <code>float</code> <p>Defaults to 1e-3.</p> <code>0.001</code> <code>metrics</code> <code>List[str]</code> <p>Validation metrics. Defaults to [\"accuracy\"].</p> <code>['accuracy']</code> <code>imagenet_pretrained</code> <code>bool</code> <p>Load a model pretrained on imagenet or not. Defaults to False.</p> <code>False</code> <code>validation_data</code> <code>Optional[tf.data.Dataset]</code> <p>Defaults to None.</p> <code>None</code> <code>save_dir</code> <code>Optional[str]</code> <p>Directory to save the model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>tf.keras.Model: Trained model</p> Source code in <code>oodeel/models/training_funs/keras_models.py</code> <pre><code>def train_keras_app(\n    train_data: tf.data.Dataset,\n    model_name: str,\n    input_shape: tuple = None,\n    num_classes: int = None,\n    is_prepared: bool = False,\n    batch_size: int = 128,\n    epochs: int = 50,\n    loss: str = \"sparse_categorical_crossentropy\",\n    optimizer: str = \"adam\",\n    learning_rate: float = 1e-3,\n    metrics: List[str] = [\"accuracy\"],\n    imagenet_pretrained: bool = False,\n    validation_data: Optional[tf.data.Dataset] = None,\n    save_dir: Optional[str] = None,\n) -&gt; tf.keras.Model:\n\"\"\"Loads a model from keras.applications.\n    If the dataset is different from imagenet, trains on provided dataset.\n\n    Args:\n        train_data (tf.data.Dataset)\n        model_name (str): must be a model from tf.keras.applications\n        input_shape (tuple, optional): If None, infered from train_data.\n            Defaults to None.\n        num_classes (int, optional): If None, infered from train_data. Defaults to None.\n        is_prepared (bool, optional): If train_data is a pipeline already prepared\n            for training (with batch, shufle, cache etc...). Defaults to False.\n        batch_size (int, optional): Defaults to 128.\n        epochs (int, optional): Defaults to 50.\n        loss (str, optional): Defaults to\n            \"sparse_categorical_crossentropy\".\n        optimizer (str, optional): Defaults to \"adam\".\n        learning_rate (float, optional): Defaults to 1e-3.\n        metrics (List[str], optional): Validation metrics. Defaults to [\"accuracy\"].\n        imagenet_pretrained (bool, optional): Load a model pretrained on imagenet or\n            not. Defaults to False.\n        validation_data (Optional[tf.data.Dataset], optional): Defaults to None.\n        save_dir (Optional[str], optional): Directory to save the model.\n            Defaults to None.\n\n    Returns:\n        tf.keras.Model: Trained model\n    \"\"\"\n\n    # Prepare model\n    if imagenet_pretrained:\n        input_shape = (224, 224, 3)\n        backbone = getattr(tf.keras.applications, model_name)(\n            include_top=False, weights=\"imagenet\", input_shape=input_shape\n        )\n        num_classes = 1000\n    else:\n        if input_shape is None:\n            input_shape = dataset_image_shape(train_data)\n        if num_classes is None:\n            classes = train_data.map(lambda x, y: y).unique()\n            num_classes = len(list(classes.as_numpy_iterator()))\n\n        if model_name != \"resnet18\":\n            backbone = getattr(tf.keras.applications, model_name)(\n                include_top=False, weights=None, input_shape=input_shape\n            )\n\n    if model_name != \"resnet18\":\n        features = Flatten()(backbone.layers[-1].output)\n        output = Dense(\n            num_classes,\n            kernel_initializer=\"glorot_normal\",\n            bias_initializer=\"zeros\",\n            activation=\"softmax\",\n        )(features)\n        model = tf.keras.Model(backbone.layers[0].input, output)\n    else:\n        ResNet18, _ = Classifiers.get(\"resnet18\")\n        model = ResNet18(input_shape, classes=num_classes, weights=None)\n\n    # Prepare data\n\n    if not is_prepared:\n        padding = 4\n        image_size = input_shape[0]\n        target_size = image_size + padding * 2\n\n        def _augment_fn(images, labels):\n            images = tf.image.pad_to_bounding_box(\n                images, padding, padding, target_size, target_size\n            )\n            images = tf.image.random_crop(images, (image_size, image_size, 3))\n            images = tf.image.random_flip_left_right(images)\n            return images, labels\n\n        n_samples = len(train_data)\n        train_data = (\n            train_data.map(\n                _augment_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n            )\n            .shuffle(n_samples)\n            .batch(batch_size)\n        )\n\n        if validation_data is not None:\n            validation_data = validation_data.batch(batch_size)\n\n    # Prepare callbacks\n    model_checkpoint_callback = []\n\n    if save_dir is not None:\n        checkpoint_filepath = save_dir\n        model_checkpoint_callback.append(\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath=checkpoint_filepath,\n                save_weights_only=False,\n                monitor=\"val_accuracy\",\n                mode=\"max\",\n                save_best_only=True,\n            )\n        )\n\n    if len(model_checkpoint_callback) == 0:\n        model_checkpoint_callback = None\n\n    # Prepare learning rate scheduler and optimizer\n    n_steps = dataset_cardinality(train_data) * epochs\n    values = list(learning_rate * np.array([1, 0.1, 0.01]))\n    boundaries = list(np.round(n_steps * np.array([1 / 3, 2 / 3])).astype(int))\n\n    lr_scheduler = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n        boundaries, values\n    )\n    config = {\"class_name\": optimizer, \"config\": {\"learning_rate\": lr_scheduler}}\n    keras_optimizer = tf.keras.optimizers.get(config)\n\n    model.compile(loss=loss, optimizer=keras_optimizer, metrics=metrics)\n\n    model.fit(\n        train_data,\n        validation_data=validation_data,\n        epochs=epochs,\n        callbacks=model_checkpoint_callback,\n    )\n\n    return model\n</code></pre>"},{"location":"api/training_funs/#oodeel.models.training_funs.torch_models","title":"<code>torch_models</code>","text":""},{"location":"api/training_funs/#oodeel.models.training_funs.torch_models.run_tf_on_cpu","title":"<code>run_tf_on_cpu()</code>","text":"<p>Run tensorflow on cpu device. It prevents tensorflow from allocating the totality of the GPU so that some VRAM remain free to train torch models.</p> <p>/!\\ This function needs to be called BEFORE loading the tfds dataset!</p> Source code in <code>oodeel/models/training_funs/torch_models.py</code> <pre><code>def run_tf_on_cpu():\n\"\"\"Run tensorflow on cpu device.\n    It prevents tensorflow from allocating the totality of the GPU so that\n    some VRAM remain free to train torch models.\n\n    /!\\\\ This function needs to be called BEFORE loading the tfds dataset!\n    \"\"\"\n    tf.config.set_visible_devices([], \"GPU\")\n</code></pre>"},{"location":"api/training_funs/#oodeel.models.training_funs.torch_models.train_torch_model","title":"<code>train_torch_model(train_data, model_name='resnet18', batch_size=128, epochs=50, loss='CrossEntropyLoss', optimizer='Adam', learning_rate=0.001, imagenet_pretrained=False, validation_data=None, save_dir=None, cuda_idx=0)</code>","text":"<p>Load a model from torchvision.models and train it on a tfds dataset.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>tf.data.Dataset</code> <p>description</p> required <code>model_name</code> <code>str</code> <p>description</p> <code>'resnet18'</code> <code>batch_size</code> <code>int</code> <p>description. Defaults to 128.</p> <code>128</code> <code>epochs</code> <code>int</code> <p>description. Defaults to 50.</p> <code>50</code> <code>loss</code> <code>str</code> <p>description. Defaults to \"crossentropy\".</p> <code>'CrossEntropyLoss'</code> <code>optimizer</code> <code>str</code> <p>description. Defaults to \"adam\".</p> <code>'Adam'</code> <code>learning_rate</code> <code>float</code> <p>description. Defaults to 1e-3.</p> <code>0.001</code> <code>metrics</code> <p>description. Defaults to [\"accuracy\"].</p> required <code>imagenet_pretrained</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>validation_data</code> <code>Optional[tf.data.Dataset]</code> <p>description. Defaults to None.</p> <code>None</code> <code>cuda_idx</code> <code>int</code> <p>description. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>nn.Module</code> <p>trained model</p> Source code in <code>oodeel/models/training_funs/torch_models.py</code> <pre><code>def train_torch_model(\n    train_data: tf.data.Dataset,\n    model_name: str = \"resnet18\",\n    batch_size: int = 128,\n    epochs: int = 50,\n    loss: str = \"CrossEntropyLoss\",\n    optimizer: str = \"Adam\",\n    learning_rate: float = 1e-3,\n    imagenet_pretrained: bool = False,\n    validation_data: Optional[tf.data.Dataset] = None,\n    save_dir: Optional[str] = None,\n    cuda_idx: int = 0,\n) -&gt; nn.Module:\n\"\"\"\n    Load a model from torchvision.models and train it on a tfds dataset.\n\n    Args:\n        train_data: _description_\n        model_name: _description_\n        batch_size: _description_. Defaults to 128.\n        epochs: _description_. Defaults to 50.\n        loss: _description_. Defaults to \"crossentropy\".\n        optimizer: _description_. Defaults to \"adam\".\n        learning_rate: _description_. Defaults to 1e-3.\n        metrics: _description_. Defaults to [\"accuracy\"].\n        imagenet_pretrained: _description_. Defaults to False.\n        validation_data: _description_. Defaults to None.\n        cuda_idx: _description_. Defaults to 0.\n\n    Returns:\n        trained model\n    \"\"\"\n    # device\n    device = torch.device(f\"cuda:{cuda_idx}\" if cuda_idx is not None else \"cpu\")\n\n    # Prepare model\n    input_shape = dataset_image_shape(train_data)\n    classes = train_data.map(lambda x, y: y).unique()\n    num_classes = len(list(classes.as_numpy_iterator()))\n\n    model = getattr(torchvision.models, model_name)(\n        num_classes=num_classes, pretrained=imagenet_pretrained\n    ).to(device)\n\n    # Prepare data\n    padding = 4\n    image_size = input_shape[0]\n    target_size = image_size + padding * 2\n\n    def _augment_fn(images, labels):\n\n        images = tf.image.pad_to_bounding_box(\n            images, padding, padding, target_size, target_size\n        )\n        images = tf.image.random_crop(images, (image_size, image_size, 3))\n        images = tf.image.random_flip_left_right(images)\n        return images, labels\n\n    n_samples = len(train_data)\n    train_data = (\n        train_data.map(_augment_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        .shuffle(n_samples)\n        .batch(batch_size)\n    )\n\n    if validation_data is not None:\n        validation_data = validation_data.batch(batch_size)\n\n    # define optimizer and learning rate scheduler\n    n_steps = len(train_data) * epochs\n    boundaries = list(np.round(n_steps * np.array([1 / 3, 2 / 3])).astype(int))\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    lr_scheduler = optim.lr_scheduler.MultiStepLR(\n        optimizer, milestones=boundaries, gamma=0.1\n    )\n\n    # define loss\n    criterion = getattr(nn, loss)()\n\n    # train\n    model = _train(\n        model,\n        train_data,\n        validation_data=validation_data,\n        epochs=epochs,\n        criterion=criterion,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        save_dir=save_dir,\n        device=device,\n    )\n    return model\n</code></pre>"},{"location":"api/training_funs/#oodeel.models.training_funs.toy","title":"<code>toy</code>","text":""},{"location":"api/training_funs/#oodeel.models.training_funs.toy.train_convnet_classifier","title":"<code>train_convnet_classifier(train_data, input_shape=None, num_classes=None, is_prepared=False, batch_size=128, epochs=50, loss='sparse_categorical_crossentropy', optimizer='adam', learning_rate=0.001, metrics=['accuracy'], validation_data=None, save_dir=None)</code>","text":"<p>Loads a model from keras.applications. If the dataset is different from imagenet, trains on provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>tf.data.Dataset</code> <p>description</p> required <code>model_name</code> <p>description</p> required <code>batch_size</code> <code>int</code> <p>description. Defaults to 128.</p> <code>128</code> <code>epochs</code> <code>int</code> <p>description. Defaults to 50.</p> <code>50</code> <code>loss</code> <code>str</code> <p>description. Defaults to \"categorical_crossentropy\".</p> <code>'sparse_categorical_crossentropy'</code> <code>optimizer</code> <code>str</code> <p>description. Defaults to \"adam\".</p> <code>'adam'</code> <code>learning_rate</code> <code>float</code> <p>description. Defaults to 1e-3.</p> <code>0.001</code> <code>metrics</code> <code>List[str]</code> <p>description. Defaults to [\"accuracy\"].</p> <code>['accuracy']</code> <code>imagenet_pretrained</code> <p>description. Defaults to False.</p> required <code>validation_data</code> <code>Optional[tf.data.Dataset]</code> <p>description. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>trained model</p> Source code in <code>oodeel/models/training_funs/toy.py</code> <pre><code>def train_convnet_classifier(\n    train_data: tf.data.Dataset,\n    input_shape: tuple = None,\n    num_classes: int = None,\n    is_prepared: bool = False,\n    batch_size: int = 128,\n    epochs: int = 50,\n    loss: str = \"sparse_categorical_crossentropy\",\n    optimizer: str = \"adam\",\n    learning_rate: float = 1e-3,\n    metrics: List[str] = [\"accuracy\"],\n    validation_data: Optional[tf.data.Dataset] = None,\n    save_dir: Optional[str] = None,\n) -&gt; tf.keras.Model:\n\"\"\"\n    Loads a model from keras.applications.\n    If the dataset is different from imagenet, trains on provided dataset.\n\n    Args:\n        train_data: _description_\n        model_name: _description_\n        batch_size: _description_. Defaults to 128.\n        epochs: _description_. Defaults to 50.\n        loss: _description_. Defaults to \"categorical_crossentropy\".\n        optimizer: _description_. Defaults to \"adam\".\n        learning_rate: _description_. Defaults to 1e-3.\n        metrics: _description_. Defaults to [\"accuracy\"].\n        imagenet_pretrained: _description_. Defaults to False.\n        validation_data: _description_. Defaults to None.\n\n    Returns:\n        trained model\n    \"\"\"\n    # Prepare model\n\n    assert (num_classes is not None) and (\n        input_shape is not None\n    ), \"Please specify num_classes and input_shape\"\n\n    model = tf.keras.Sequential(\n        [\n            # keras.Input(shape=input_shape),\n            Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n            MaxPooling2D(pool_size=(2, 2)),\n            Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n            MaxPooling2D(pool_size=(2, 2)),\n            Flatten(),\n            Dropout(0.5),\n            Dense(num_classes, activation=\"softmax\"),\n        ]\n    )\n\n    # Prepare data\n    if not is_prepared:\n        padding = 4\n        image_size = input_shape[0]\n        nb_channels = input_shape[2]\n        target_size = image_size + padding * 2\n\n        def _augment_fn(images, labels):\n            images = tf.image.pad_to_bounding_box(\n                images, padding, padding, target_size, target_size\n            )\n            images = tf.image.random_crop(images, (image_size, image_size, nb_channels))\n            images = tf.image.random_flip_left_right(images)\n            return images, labels\n\n        n_samples = len(train_data)\n        train_data = (\n            train_data.map(\n                _augment_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n            )\n            .shuffle(n_samples)\n            .batch(batch_size)\n        )\n\n        if validation_data is not None:\n            validation_data = validation_data.batch(batch_size)\n\n    # Prepare callbacks\n    model_checkpoint_callback = []\n\n    if save_dir is not None:\n        checkpoint_filepath = save_dir\n        model_checkpoint_callback.append(\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath=checkpoint_filepath,\n                save_weights_only=False,\n                monitor=\"val_accuracy\",\n                mode=\"max\",\n                save_best_only=True,\n            )\n        )\n\n    if len(model_checkpoint_callback) == 0:\n        model_checkpoint_callback = None\n\n    # Prepare learning rate scheduler and optimizer\n    n_steps = dataset_cardinality(train_data) * epochs\n    values = list(learning_rate * np.array([1, 0.1, 0.01]))\n    boundaries = list(np.round(n_steps * np.array([1 / 3, 2 / 3])).astype(int))\n\n    lr_scheduler = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n        boundaries, values\n    )\n    config = {\"class_name\": optimizer, \"config\": {\"learning_rate\": lr_scheduler}}\n    keras_optimizer = tf.keras.optimizers.get(config)\n\n    model.compile(loss=loss, optimizer=keras_optimizer, metrics=metrics)\n\n    model.fit(\n        train_data,\n        validation_data=validation_data,\n        epochs=epochs,\n        callbacks=model_checkpoint_callback,\n    )\n\n    return model\n</code></pre>"},{"location":"notebooks/demo_experiment/","title":"Get started","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());  <pre><code>%load_ext autoreload\n%env PYTHONPATH=../\n\nimport sys\nsys.path.append(\"../\")\nimport pprint\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\nimport numpy as np\nfrom tensorflow import keras\nfrom oodeel.methods import MLS, DKNN, ODIN\nfrom oodeel.methods import DKNN\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\n\nfrom oodeel.eval.metrics import bench_metrics, get_curve\nfrom oodeel.datasets import OODDataset\n\nfrom sklearn.metrics import *\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npp = pprint.PrettyPrinter()\n</code></pre> <pre>\n<code>env: PYTHONPATH=../\n</code>\n</pre> <pre>\n<code>/Users/paul/workspace/oodeel/oodeel_dev_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</code>\n</pre> <pre><code>%autoreload 2\n\noods_in = OODDataset('mnist', split=\"test\")\noods_out = OODDataset('fashion_mnist', split=\"test\")\noods_fit = OODDataset('mnist', split=\"train\")\n\ndef preprocess_fn(*inputs):\n    x = inputs[0] / 255\n    return tuple([x] + list(inputs[1:]))\n\n\nbatch_size = 128\nds_in = oods_in.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn)\nds_out = oods_out.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn)\nds_fit = oods_fit.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn, shuffle=True)\n</code></pre> <pre>\n<code>Metal device set to: AMD Radeon Pro 5500M\n\nsystemMemory: 64.00 GB\nmaxCacheSize: 3.99 GB\n\nWARNING:tensorflow:From /Users/paul/workspace/oodeel/oodeel_dev_env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n</code>\n</pre> <pre>\n<code>WARNING:tensorflow:From /Users/paul/workspace/oodeel/oodeel_dev_env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\nInstructions for updating:\nLambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n</code>\n</pre> <pre><code>from oodeel.models.training_funs import train_convnet_classifier\n\ntry:\n    model = tf.keras.models.load_model(\"../saved_models/mnist_model\")\nexcept OSError:\n\n\n    train_config = {\n        \"input_shape\": (28, 28, 1),\n        \"num_classes\": 10,\n        \"is_prepared\": True,\n        \"batch_size\": 128,\n        \"epochs\": 5,\n        \"save_dir\": \"../saved_models/mnist_model\",\n        \"validation_data\": ds_in #ds_in is actually the test set of MNIST\n    }\n\n    model = train_convnet_classifier(ds_fit, **train_config) #ds_fit is actually the train set of MNIST\n</code></pre> <pre>\n<code>Epoch 1/5\n469/469 [==============================] - ETA: 0s - loss: 0.3287 - accuracy: 0.9023</code>\n</pre> <pre>\n<code>WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n</code>\n</pre> <pre>\n<code>INFO:tensorflow:Assets written to: ../saved_models/mnist_model/assets\n</code>\n</pre> <pre>\n<code>INFO:tensorflow:Assets written to: ../saved_models/mnist_model/assets\n</code>\n</pre> <pre>\n<code>469/469 [==============================] - 16s 32ms/step - loss: 0.3287 - accuracy: 0.9023 - val_loss: 0.0865 - val_accuracy: 0.9729\nEpoch 2/5\n469/469 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9705</code>\n</pre> <pre>\n<code>WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n</code>\n</pre> <pre>\n<code>INFO:tensorflow:Assets written to: ../saved_models/mnist_model/assets\n</code>\n</pre> <pre>\n<code>INFO:tensorflow:Assets written to: ../saved_models/mnist_model/assets\n</code>\n</pre> <pre>\n<code>469/469 [==============================] - 13s 28ms/step - loss: 0.0977 - accuracy: 0.9705 - val_loss: 0.0555 - val_accuracy: 0.9828\nEpoch 3/5\n469/469 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9766</code>\n</pre> <pre>\n<code>WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n</code>\n</pre> <pre>\n<code>INFO:tensorflow:Assets written to: ../saved_models/mnist_model/assets\n</code>\n</pre> <pre>\n<code>INFO:tensorflow:Assets written to: ../saved_models/mnist_model/assets\n</code>\n</pre> <pre>\n<code>469/469 [==============================] - 14s 30ms/step - loss: 0.0783 - accuracy: 0.9766 - val_loss: 0.0513 - val_accuracy: 0.9843\nEpoch 4/5\n469/469 [==============================] - 10s 22ms/step - loss: 0.0747 - accuracy: 0.9778 - val_loss: 0.0510 - val_accuracy: 0.9839\nEpoch 5/5\n469/469 [==============================] - 11s 24ms/step - loss: 0.0734 - accuracy: 0.9776 - val_loss: 0.0509 - val_accuracy: 0.9835\n</code>\n</pre> <pre><code>%autoreload 2\n\n\noodmodel = MLS()\noodmodel.fit(model)\nscores_in = oodmodel.score(ds_in)\nscores_out = oodmodel.score(ds_out)\n\n\nmetrics = bench_metrics(\n    (scores_in, scores_out),  \n    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n    threshold = -5\n    )\n\npp.pprint(metrics)\n</code></pre> <pre>\n<code>{'accuracy_score': 0.92835,\n 'auroc': 0.9792125049999999,\n 'fpr95tpr': 0.0613,\n 'roc_auc_score': 0.97921253}\n</code>\n</pre> <pre><code>%autoreload 2\n\n\noodmodel = DKNN()\noodmodel.fit(model, ds_fit.take(10000))\nscores_in = oodmodel.score(ds_in.take(1000))\nscores_out = oodmodel.score(ds_out.take(1000))\n\n\nmetrics = bench_metrics(\n    (scores_in, scores_out),  \n    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n    threshold = -5\n    )\n\npp.pprint(metrics)\n</code></pre> <pre>\n<code>{'accuracy_score': 0.5,\n 'auroc': 0.9984382249999999,\n 'fpr95tpr': 0.0061,\n 'roc_auc_score': 0.998438245}\n</code>\n</pre> <pre><code>%autoreload 2\nfrom oodeel.methods import ODIN\n\noodmodel = ODIN()\noodmodel.fit(model)\nscores_in = oodmodel.score(ds_in)\nscores_out = oodmodel.score(ds_out)\n\n\nmetrics = bench_metrics(\n    (scores_in, scores_out),  \n    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n    threshold = -5\n    )\n\npp.pprint(metrics)\n</code></pre> <pre>\n<code>{'accuracy_score': 0.5,\n 'auroc': 0.9909027650000001,\n 'fpr95tpr': 0.0372,\n 'roc_auc_score': 0.990903005}\n</code>\n</pre> <pre><code>%autoreload 2\n\noods_test = OODDataset('mnist', split=\"test\")\noods_train = OODDataset('mnist', split=\"train\")\n\nbatch_size = 128\ninc_labels = [0, 1, 2, 3, 4]\noods_train, _ = oods_train.assign_ood_labels_by_class(in_labels=inc_labels)\noods_in, oods_out = oods_test.assign_ood_labels_by_class(in_labels=inc_labels)\n\n\ndef preprocess_fn(*inputs):\n    x = inputs[0] / 255\n    return tuple([x] + list(inputs[1:]))\n\nds_train = oods_train.prepare(batch_size=batch_size, preprocess_fn=preprocess_fn, shuffle=True)\nds_in = oods_in.prepare(batch_size=batch_size, with_ood_labels=False, preprocess_fn=preprocess_fn)\nds_out = oods_out.prepare(batch_size=batch_size, with_ood_labels=False, preprocess_fn=preprocess_fn)\n</code></pre> <pre><code>%autoreload 2\nfrom oodeel.models.training_funs import train_convnet_classifier\n\ntrain_config = {\n    \"input_shape\": (28, 28, 1),\n    \"num_classes\": len(inc_labels),\n    \"is_prepared\": True,\n    \"batch_size\": 128,\n    \"epochs\": 5\n}\n\nmodel = train_convnet_classifier(ds_train, **train_config)\n</code></pre> <pre>\n<code>Epoch 1/5\n240/240 [==============================] - 9s 32ms/step - loss: 0.2136 - accuracy: 0.9347\nEpoch 2/5\n240/240 [==============================] - 6s 24ms/step - loss: 0.0500 - accuracy: 0.9850\nEpoch 3/5\n240/240 [==============================] - 7s 29ms/step - loss: 0.0411 - accuracy: 0.9874\nEpoch 4/5\n240/240 [==============================] - 5s 20ms/step - loss: 0.0391 - accuracy: 0.9874\nEpoch 5/5\n240/240 [==============================] - 6s 23ms/step - loss: 0.0386 - accuracy: 0.9883\n</code>\n</pre> <pre><code>%autoreload 2\n\noodmodel = MLS()\noodmodel.fit(model)\nscores_in = oodmodel.score(ds_in)\nscores_out = oodmodel.score(ds_out)\n\n\nmetrics = bench_metrics(\n    (scores_in, scores_out),  \n    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n    threshold = -5\n    )\n\npp.pprint(metrics)\n</code></pre> <pre>\n<code>{'accuracy_score': 0.8827,\n 'auroc': 0.9456968323399055,\n 'fpr95tpr': 0.25179996108192254,\n 'roc_auc_score': 0.9456956314117804}\n</code>\n</pre> <pre><code>%autoreload 2\n\noodmodel = DKNN()\noodmodel.fit(model, ds_train.take(10000))\nscores_in = oodmodel.score(ds_in.take(1000))\nscores_out = oodmodel.score(ds_out.take(1000))\n\n\nmetrics = bench_metrics(\n    (scores_in, scores_out),  \n    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n    threshold = -5\n    )\n\npp.pprint(metrics)\n</code></pre> <pre>\n<code>{'accuracy_score': 0.4861,\n 'auroc': 0.967651319645875,\n 'fpr95tpr': 0.14127262113251604,\n 'roc_auc_score': 0.9676495582846246}\n</code>\n</pre> <pre><code>%autoreload 2\n\n#x_test, y_id = data_handler.convert_to_numpy(x_id)\n\noodmodel = ODIN()\noodmodel.fit(model)\nscores_in = oodmodel.score(ds_in)\nscores_out = oodmodel.score(ds_out)\n\n\nmetrics = bench_metrics(\n    (scores_in, scores_out),  \n    metrics = [\"auroc\", \"fpr95tpr\", accuracy_score, roc_auc_score], \n    threshold = -5\n    )\n\npp.pprint(metrics)\n</code></pre> <pre>\n<code>{'accuracy_score': 0.4861,\n 'auroc': 0.9244589988927043,\n 'fpr95tpr': 0.3957968476357268,\n 'roc_auc_score': 0.9244590589391105}\n</code>\n</pre>"},{"location":"notebooks/demo_experiment/#two-datasets-experiment","title":"Two datasets experiment","text":""},{"location":"notebooks/demo_experiment/#mls","title":"MLS","text":""},{"location":"notebooks/demo_experiment/#dknn","title":"DKNN","text":""},{"location":"notebooks/demo_experiment/#odin","title":"ODIN","text":""},{"location":"notebooks/demo_experiment/#single-dataset-experiment","title":"Single dataset experiment","text":"<p>(Leave-\\(k\\)-classes-out training). First, we need to define a training function</p>"},{"location":"notebooks/demo_experiment/#mls_1","title":"MLS","text":""},{"location":"notebooks/demo_experiment/#dknn_1","title":"DKNN","text":""},{"location":"notebooks/demo_experiment/#odin_1","title":"ODIN","text":""},{"location":"notebooks/dknn_benchmark/","title":"DKNN benchmark","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());  <pre><code>%load_ext autoreload\n%autoreload 2\n\n%env TFDS_DATA_DIR=/Users/paul/scratch/tensorflow_datasets\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append(\"../\")\n\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom oodeel.methods import DKNN\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import *\n\nfrom oodeel.eval.metrics import bench_metrics, get_curve\nfrom oodeel.datasets import DataHandler\n</code></pre> <pre>\n<code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nenv: TFDS_DATA_DIR=/Users/paul/scratch/tensorflow_datasets\n</code>\n</pre> <p>Note: In <code>oodeel.methods.odin</code>, the import <code>from keras.utils.generic_utils import get_custom_objects</code> should be replaced with <code>from keras.utils import get_custom_objects</code></p> <pre><code>data_handler = DataHandler()\nnormalize = lambda x: x / 255\nmnist_ds = data_handler.load_tfds('mnist', preprocess=True, preprocessing_fun=normalize)\n\n# train / test set\nx_train, _ = data_handler.filter_tfds(mnist_ds['train'], inc_labels=list(range(5)))\nx_test, x_ood = data_handler.filter_tfds(mnist_ds['test'], inc_labels=list(range(5)))\n\n# id / ood set\nx_id = x_test\n</code></pre> <pre><code>load = True\ncheckpoint_path = '../saved_models/mnist_5'\n\nif load:\n    model = tf.keras.models.load_model(checkpoint_path)\nelse:\n    # define model\n    num_classes = 5\n    input_shape = (28, 28, 1)\n\n    model = keras.Sequential(\n        [\n            keras.Input(shape=input_shape),\n            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n            layers.MaxPooling2D(pool_size=(2, 2)),\n            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n            layers.MaxPooling2D(pool_size=(2, 2)),\n            layers.Flatten(),\n            layers.Dropout(0.5),\n            layers.Dense(num_classes, activation=\"softmax\"),\n        ]\n    )\n    model.summary()\n\n    # train model\n    batch_size = 128\n    epochs = 15\n\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.fit(x_train.batch(batch_size), validation_data=x_test.batch(batch_size), epochs=epochs)\n    model.save(checkpoint_path)\n</code></pre> <pre><code>batch_size = 128\n\noodmodel = DKNN(nearest=50)\noodmodel.fit(model, x_train.take(10000).batch(batch_size))\nscores_id = oodmodel.score(x_id.take(10000).batch(batch_size))\nscores_ood = oodmodel.score(x_ood.take(10000).batch(batch_size))\n\n# auroc / fpr95\nscores = np.concatenate([scores_id, scores_ood])\nlabels= np.array([0] * len(scores_id) + [1] * len(scores_ood))\nfpr, tpr = get_curve(scores, labels)\nmetrics = bench_metrics(\n    scores, labels,\n    metrics=[\"auroc\", \"fpr95tpr\"],\n)\nmetrics_first_vs_last = pd.Series(metrics, name='mnist-0-4-vs-5-10')\nprint(metrics_first_vs_last)\n\n# plot hists / roc\nplt.figure(figsize=(9,3))\nplt.subplot(121)\nplt.hist((scores_ood, scores_id), bins=30, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\nplt.legend()\nplt.subplot(122)\nplt.plot(fpr, tpr)\nplt.show()\n</code></pre> <pre>\n<code>auroc       0.974568\nfpr95tpr    0.115587\nName: mnist-0-4-vs-5-10, dtype: float64\n</code>\n</pre> <pre><code>\n</code></pre> <pre><code>data_handler = DataHandler()\nnormalize = lambda x: x / 255\nds1 = data_handler.load_tfds('mnist', preprocess=True, preprocessing_fun=normalize)\nds2 = data_handler.load_tfds('fashion_mnist', preprocess=True, preprocessing_fun=normalize)\n\n# train / test set\nx_train, x_test = ds1['train'], ds1['test']\n# id / ood set\nx_id, x_ood = ds1[\"test\"], ds2[\"test\"]\nx_test_ood = data_handler.merge_tfds(x_id.take(1000), x_ood.take(1000), shuffle='True')\n</code></pre> <pre>\n<code>2023-02-10 15:52:51.708648: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</code>\n</pre> <pre><code>load = True\ncheckpoint_path = '../saved_models/mnist_10'\n\nif load:\n    model = tf.keras.models.load_model(checkpoint_path)\nelse:\n    # define model\n    num_classes = 10\n    input_shape = (28, 28, 1)\n\n    model = keras.Sequential(\n        [\n            keras.Input(shape=input_shape),\n            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n            layers.MaxPooling2D(pool_size=(2, 2)),\n            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n            layers.MaxPooling2D(pool_size=(2, 2)),\n            layers.Flatten(),\n            layers.Dropout(0.5),\n            layers.Dense(num_classes, activation=\"softmax\"),\n        ]\n    )\n    model.summary()\n\n    # train model\n    batch_size = 128\n    epochs = 15\n\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.fit(x_train.batch(batch_size), validation_data=x_test.batch(batch_size), epochs=epochs)\n    model.save(checkpoint_path)\n</code></pre> <pre><code>oodmodel = DKNN(nearest=50)\noodmodel.fit(model, x_train.take(10000).batch(batch_size))\nscores_id = oodmodel.score(x_id.take(10000).batch(batch_size)) \nscores_ood = oodmodel.score(x_ood.take(10000).batch(batch_size))\n\n# auroc / fpr95\nscores = np.concatenate([scores_id, scores_ood])\nlabels= np.array([0] * len(scores_id) + [1] * len(scores_ood))\nfpr, tpr = get_curve(scores, labels)\nmetrics = bench_metrics(\n    scores, labels,\n    metrics=[\"auroc\", \"fpr95tpr\"]\n)\nmetrics_fashion = pd.Series(metrics, name='mnist-10-vs-fashion')\nprint(metrics_fashion)\n\n# plot hists / roc\nplt.figure(figsize=(9,3))\nplt.subplot(121)\nplt.hist((scores_ood, scores_id), bins=30, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\nplt.legend()\nplt.subplot(122)\nplt.plot(fpr, tpr)\nplt.show()\n</code></pre> <pre>\n<code>2023-02-10 16:04:45.311473: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</code>\n</pre> <pre>\n<code>auroc       0.997319\nfpr95tpr    0.009200\nName: mnist-10-vs-fashion, dtype: float64\n</code>\n</pre> <pre><code>data_handler = DataHandler()\nnormalize = lambda x: x / 255\nds1 = data_handler.load_tfds('cifar10', preprocess=True, preprocessing_fun=normalize)\nds2 = data_handler.load_tfds('svhn_cropped', preprocess=True, preprocessing_fun=normalize)\n\n# train / test set\nx_train, x_test = ds1['train'], ds1['test']\n# id set\nx_id= ds1[\"test\"]\n\n# load model\n# (to train the model, run: python -m notebooks.train_cifar10)\nmodel = tf.keras.models.load_model(\"../saved_models/cifar10_dknn/\")\n\n# model = tf.keras.models.load_model(\"../saved_models/cifar10/\")\n# best: cifar10 =&gt; 200 epochs, cosine scheduler, [0, 1]\n</code></pre> <pre>\n<code>Downloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /Users/paul/scratch/tensorflow_datasets/cifar10/3.0.2...\n</code>\n</pre> <pre>\n<code>Dl Completed...: 0 url [00:00, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:00&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:01&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:02&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:03&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:04&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:04&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:05&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:05&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:05&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:06&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:07&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:07&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:07&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:08&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:08&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:08&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:09&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:09&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:09&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:10&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:10&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:10&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:11&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:11&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:12&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:12&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:12&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:12&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:13&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:13&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:13&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:14&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:14&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:15&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:15&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:15&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:16&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:16&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:16&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:17&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:17&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:17&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:18&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:18&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:18&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:19&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:19&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:19&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:19&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:20&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:20&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:21&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:21&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:22&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:22&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:23&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:23&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:24&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:25&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:25&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:26&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:26&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:27&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:28&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:28&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:29&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:29&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:30&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:30&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:31&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:31&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:32&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:32&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:33&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:34&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:34&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:34&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:35&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:35&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:35&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:36&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:36&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:36&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:37&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:37&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:37&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:38&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:39&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:39&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:40&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:40&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:41&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:42&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:42&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:42&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:43&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:43&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:43&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:44&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:44&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:44&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:45&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:45&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:46&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:46&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:46&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:47&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:47&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:48&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:49&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:49&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:49&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:50&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:50&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:50&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:51&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:51&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:52&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:52&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:52&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:53&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:53&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:54&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:54&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:55&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:55&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:56&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:56&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:57&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:57&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:58&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:58&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:59&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:59&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:59&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:00&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:00&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:01&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:01&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:02&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:02&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:02&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:03&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:03&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:04&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:04&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:04&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:05&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:05&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:06&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:06&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:06&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:07&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:07&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:07&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:08&lt;?, ? url/s]\nDl Completed...:   0%|          | 0/1 [01:08&lt;?, ? url/s]\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [01:08&lt;00:00, 68.20s/ url]\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [01:08&lt;00:00, 68.20s/ url]\n\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [01:09&lt;00:00, 68.20s/ url]\nExtraction completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [01:09&lt;00:00, 69.98s/ file]\nDl Size...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162/162 [01:09&lt;00:00,  2.31 MiB/s]\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [01:09&lt;00:00, 69.98s/ url]\n\n</code>\n</pre> <pre>\n<code>Dataset cifar10 downloaded and prepared to /Users/paul/scratch/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\nDownloading and preparing dataset 1.47 GiB (download: 1.47 GiB, generated: Unknown size, total: 1.47 GiB) to /Users/paul/scratch/tensorflow_datasets/svhn_cropped/3.0.0...\n</code>\n</pre> <pre>\n<code>Dl Completed...:   0%|          | 0/3 [03:22&lt;?, ? url/s]</code>\n</pre> <pre><code># ood dataset\nds2 = data_handler.load_tfds('svhn_cropped', preprocess=True, preprocessing_fun=normalize)\nx_ood = ds2['test']\n\noodmodel = DKNN(nearest=50, output_layers_id=['pool1'])\noodmodel.fit(model, x_train.take(10000).batch(batch_size))\nscores_id = oodmodel.score(x_id.take(10000).batch(batch_size))\nscores_ood = oodmodel.score(x_ood.take(10000).batch(batch_size))\n\n# auroc / fpr95\nscores = np.concatenate([scores_id, scores_ood])\nlabels= np.array([0] * len(scores_id) + [1] * len(scores_ood))\nfpr, tpr = get_curve(scores, labels)\nmetrics = bench_metrics(\n    scores, labels,\n    metrics=[\"auroc\", \"fpr95tpr\"]\n)\nmetrics_cifar_svhn = pd.Series(metrics, name='cifar-vs-svhn')\nprint(metrics_cifar_svhn)\n\n# plot hists / roc\nplt.figure(figsize=(9,3))\nplt.subplot(121)\nplt.hist((scores_ood, scores_id), bins=30, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\nplt.legend()\nplt.subplot(122)\nplt.plot(fpr, tpr)\nplt.show()\n</code></pre> <pre>\n<code>2023-02-04 01:04:37.096322: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</code>\n</pre> <pre>\n<code>auroc       0.865995\nfpr95tpr    0.370100\nName: cifar-vs-svhn, dtype: float64\n</code>\n</pre> <pre><code>import tensorflow as tf\nLSUN_root = '/data/home/yannick.prudent/ood_dataset/LSUN'\nlsun_ds = tf.keras.utils.image_dataset_from_directory(\n    LSUN_root,\n    image_size=(32, 32),\n    shuffle=False,\n    batch_size=None\n)\n# ood dataset\nx_ood = lsun_ds\n\noodmodel = DKNN(nearest=50, output_layers_id=['pool1'])\noodmodel.fit(model, x_train.take(10000))\nscores_id, scores_ood = oodmodel.score([x_id.take(10000), x_ood.take(10000)])\n\n# auroc / fpr95\nscores = np.concatenate([scores_id, scores_ood])\nlabels= np.array([0] * len(scores_id) + [1] * len(scores_ood))\nfpr, tpr = get_curve(scores, labels)\nmetrics = bench_metrics(\n    scores, labels,\n    metrics=[\"auroc\", \"fpr95tpr\"]\n)\nmetrics_cifar_lsun = pd.Series(metrics, name='cifar-vs-lsun')\nprint(metrics_cifar_lsun)\n\n# plot hists / roc\nplt.figure(figsize=(9,3))\nplt.subplot(121)\nplt.hist((scores_ood, scores_id), bins=30, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\nplt.legend()\nplt.subplot(122)\nplt.plot(fpr, tpr)\nplt.show()\n</code></pre> <pre>\n<code>Found 10000 files belonging to 1 classes.\n</code>\n</pre> <pre>\n<code>2023-02-04 01:04:43.879091: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</code>\n</pre> <pre>\n<code>auroc       0.998942\nfpr95tpr    0.002600\nName: cifar-vs-lsun, dtype: float64\n</code>\n</pre> <pre><code>import tensorflow as tf\niSUN_root = '/data/home/yannick.prudent/ood_dataset/iSUN'\nisun_ds = tf.keras.utils.image_dataset_from_directory(\n    iSUN_root,\n    image_size=(32, 32),\n    shuffle=False,\n    batch_size=None\n)\n# ood dataset\nx_ood = isun_ds\n\noodmodel = DKNN(nearest=50, output_layers_id=['pool1'])\noodmodel.fit(model, x_train.take(10000))\nscores_id, scores_ood = oodmodel.score([x_id.take(10000), x_ood.take(10000)])\n\n# auroc / fpr95\nscores = np.concatenate([scores_id, scores_ood])\nlabels= np.array([0] * len(scores_id) + [1] * len(scores_ood))\nfpr, tpr = get_curve(scores, labels)\nmetrics = bench_metrics(\n    scores, labels,\n    metrics=[\"auroc\", \"fpr95tpr\"]\n)\nmetrics_cifar_isun = pd.Series(metrics, name='cifar-vs-isun')\nprint(metrics_cifar_isun)\n\n# plot hists / roc\nplt.figure(figsize=(9,3))\nplt.subplot(121)\nplt.hist((scores_ood, scores_id), bins=30, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\nplt.legend()\nplt.subplot(122)\nplt.plot(fpr, tpr)\nplt.show()\n</code></pre> <pre>\n<code>Found 8925 files belonging to 1 classes.\n</code>\n</pre> <pre>\n<code>2023-02-04 01:21:25.941738: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</code>\n</pre> <pre>\n<code>auroc       0.997885\nfpr95tpr    0.004600\nName: cifar-vs-isun, dtype: float64\n</code>\n</pre> <pre><code>import tensorflow as tf\ntexture_root = '/data/home/yannick.prudent/ood_dataset/dtd/images'\ntexture_ds = tf.keras.utils.image_dataset_from_directory(\n    texture_root,\n    image_size=(32, 32),\n    shuffle=False,\n    batch_size=None,\n)\n# ood dataset\nx_ood = texture_ds\n\noodmodel = DKNN(nearest=50, output_layers_id=['pool1'])\noodmodel.fit(model, x_train.take(10000))\nscores_id, scores_ood = oodmodel.score([x_id.take(10000), x_ood.take(10000)])\n\n# auroc / fpr95\nscores = np.concatenate([scores_id, scores_ood])\nlabels= np.array([0] * len(scores_id) + [1] * len(scores_ood))\nfpr, tpr = get_curve(scores, labels)\nmetrics = bench_metrics(\n    scores, labels,\n    metrics=[\"auroc\", \"fpr95tpr\"]\n)\nmetrics_cifar_texture = pd.Series(metrics, name='cifar-vs-texture')\nprint(metrics_cifar_texture)\n\n# plot hists / roc\nplt.figure(figsize=(9,3))\nplt.subplot(121)\nplt.hist((scores_ood, scores_id), bins=30, color=(\"blue\", \"orange\"), label=(\"ood\", \"id\"), density=True)\nplt.legend()\nplt.subplot(122)\nplt.plot(fpr, tpr)\nplt.show()\n</code></pre> <pre>\n<code>Found 5640 files belonging to 47 classes.\n</code>\n</pre> <pre>\n<code>2023-02-04 01:29:33.423863: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n</code>\n</pre> <pre>\n<code>auroc       0.997419\nfpr95tpr    0.006700\nName: cifar-vs-texture, dtype: float64\n</code>\n</pre> <pre><code>pd.concat([metrics_first_vs_last, metrics_fashion, metrics_cifar_svhn, metrics_cifar_lsun, metrics_cifar_isun, metrics_cifar_texture], axis=1).T\n</code></pre> auroc fpr95tpr mnist-0-4-vs-5-10 0.975215 0.107803 mnist-10-vs-fashion 0.997144 0.010400 cifar-vs-svhn 0.865995 0.370100 cifar-vs-lsun 0.998942 0.002600 cifar-vs-isun 0.997885 0.004600 cifar-vs-texture 0.997419 0.006700"},{"location":"notebooks/dknn_benchmark/#imports","title":"Imports","text":""},{"location":"notebooks/dknn_benchmark/#mnist-0-4-vs-mnist-5-9","title":"MNIST (0-4) vs MNIST (5-9)","text":""},{"location":"notebooks/dknn_benchmark/#data-and-model-loading","title":"Data and model loading","text":"<p>ID data: MNIST (0-4), OOD data: MNIST (5-9)</p>"},{"location":"notebooks/dknn_benchmark/#dknn-score","title":"DKNN score","text":""},{"location":"notebooks/dknn_benchmark/#mnist-vs-fashion-mnist","title":"MNIST vs Fashion MNIST","text":""},{"location":"notebooks/dknn_benchmark/#data-and-model-loading_1","title":"Data and model loading","text":"<p>ID data: MNIST, OOD data: Fashion MNIST</p>"},{"location":"notebooks/dknn_benchmark/#dknn-score_1","title":"DKNN score","text":""},{"location":"notebooks/dknn_benchmark/#cifar-10-vs-svhn-lsun-isun-texture","title":"CIFAR-10 vs [SVHN, LSUN, iSUN, Texture]","text":""},{"location":"notebooks/dknn_benchmark/#id-data-and-model-loading","title":"ID Data and model loading","text":""},{"location":"notebooks/dknn_benchmark/#ood-data-svhn","title":"OOD data: SVHN","text":"<p>ID data: CIFAR-10, OOD data: SVHN</p>"},{"location":"notebooks/dknn_benchmark/#ood-data-lsun","title":"OOD data: LSUN","text":"<p>ID data: CIFAR-10, OOD data: LSUN</p>"},{"location":"notebooks/dknn_benchmark/#ood-data-isun","title":"OOD data: iSUN","text":"<p>ID data: CIFAR-10, OOD data: iSUN</p>"},{"location":"notebooks/dknn_benchmark/#ood-data-texture","title":"OOD data: Texture","text":"<p>ID data: CIFAR-10, OOD data: Texture</p>"},{"location":"notebooks/dknn_benchmark/#results","title":"Results","text":""},{"location":"notebooks/odin_benchmark/","title":"ODIN benchmark","text":"(function() {   function addWidgetsRenderer() {     var requireJsScript = document.createElement('script');     requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js';      var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]');     var jupyterWidgetsScript = document.createElement('script');     var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';     var widgetState;      // Fallback for older version:     try {       widgetState = mimeElement &amp;&amp; JSON.parse(mimeElement.innerHTML);        if (widgetState &amp;&amp; (widgetState.version_major &lt; 2 || !widgetState.version_major)) {         widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';       }     } catch(e) {}      jupyterWidgetsScript.src = widgetRendererSrc;      document.body.appendChild(requireJsScript);     document.body.appendChild(jupyterWidgetsScript);   }    document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());  <pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>import os\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Disable TensorFlow log messages\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n\ninput_scaling = 1 / 255\nbatch_size = 128\n\n# Load CIFAR-10 test set\nds_id = tfds.load(\"cifar10\", split=\"test\", as_supervised=True)\n\n\n# Preprocess dataset\ndef normalize(img, lbl):\n    img = tf.cast(img, tf.float32) * input_scaling\n    return img, lbl\n\n\nds_id = (\n    ds_id.map(normalize, num_parallel_calls=tf.data.AUTOTUNE)\n    .cache()\n    .batch(batch_size)\n    .prefetch(tf.data.AUTOTUNE)\n)\n</code></pre> <pre><code># Load DenseNet pretrained model\nmodel = tf.keras.models.load_model(\"./checkpoints/densenet\")\nmodel.compile(metrics=[\"accuracy\"])\n\n# Evaluate model on the CIFAR-10 test set\nout = model.evaluate(ds_id)\nprint(f\"Test accuracy on CIFAR-10: {out[1]:.3f}\")\n</code></pre> <pre><code>@tf.function\ndef load_image(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.io.decode_image(image)\n    image = normalize(image, 0)[0]\n    if image.shape[0] == 36:\n        image = tf.image.crop_to_bounding_box(image, 2, 2, 32, 32)\n    return image\n\n\ndef load_ood_dataset(dataset_name):\n    ds_ood = tf.data.Dataset.list_files(f\"./data/{dataset_name}/*/*\", shuffle=False)\n    ds_ood = ds_ood.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(\n        batch_size\n    )\n    return ds_ood\n\n\nood_name = \"LSUN\"\nds_ood = load_ood_dataset(ood_name)\n</code></pre> <pre><code># Compute OOD scores for ODIN method\n\nfrom oodeel.methods import ODIN\n\noodmodel = ODIN(temperature=1000)\noodmodel.fit(model)\n\nscores_id = oodmodel.score(ds_id)\nscores_ood = oodmodel.score(ds_ood)\n</code></pre> <pre><code># Compute and display OOD metrics\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom oodeel.eval.metrics import bench_metrics, get_curve\n\n\ndef compute_ood_metrics(scores_id, scores_ood):\n    # Compute ROC curve, AUROC, FPR and TPR.\n    scores = np.concatenate([scores_id, scores_ood])\n    labels = np.array([0] * len(scores_id) + [1] * len(scores_ood))\n    fpr, tpr = get_curve(scores, labels)\n    metrics = bench_metrics(\n        scores,\n        labels,\n        metrics=[\"auroc\", \"fpr95tpr\"],\n    )\n    return metrics, fpr, tpr\n\n\ndef display_ROC(scores_id, scores_ood, fpr, tpr):\n    plt.figure(figsize=(9, 3))\n    plt.subplot(121)\n    plt.hist((scores_id, scores_ood), bins=30, label=(\"id\", \"ood\"))\n    plt.xlabel(\"Score\")\n    plt.title(\"OOD scores\")\n    plt.legend()\n    plt.subplot(122)\n    plt.plot(fpr, tpr)\n    plt.xlabel(\"False positive rate\")\n    plt.ylabel(\"True positive rate\")\n    plt.title(\"ROC\")\n    plt.show()\n\n\nprint(\"In-distribution: CIFAR-10 / Out-of-distribution: LSUN\")\nmetrics, fpr, tpr = compute_ood_metrics(scores_id, scores_ood)\nprint(metrics)\n\ndisplay_ROC(scores_id, scores_ood, fpr, tpr)\n</code></pre> <pre><code># BONUS\n# Compare calibrated softmax outputs from original and perturbed images (single batch)\nfor img in ds_ood.take(1):\n    pass\nimg_perturbed = oodmodel._input_perturbation(img)\n\nperturbations = img_perturbed - img\nnorms = tf.norm(tf.reshape(perturbations, [perturbations.shape[0], -1]), axis=-1)\nprint(norms.numpy())\n\n# Outputs for original images\npreds = tf.nn.softmax(model.predict(img) / oodmodel.temperature).numpy()\nmaxx = preds.max(axis=-1)\nargmaxx = preds.argmax(axis=-1).squeeze()\n\n# Outputs for perturbed images\npreds_per = tf.nn.softmax(model.predict(img_perturbed) / oodmodel.temperature).numpy()\nmaxx_per = preds_per.max(axis=-1)\nargmaxx_per = preds_per.argmax(axis=-1).squeeze()\n\nprint(argmaxx)\nprint()\nprint(argmaxx_per)\nprint()\n\nprint(maxx)\nprint()\nprint(maxx_per)\nprint()\n</code></pre> <pre><code>plt.figure(figsize=(20, 15))\nplt.plot(maxx, \"*--\")\nplt.plot(maxx_per, \"*--\")\nplt.title(\"Max of calibrated softmax outputs\")\nplt.legend([\"Original\", \"Perturbed\"])\nplt.show()\n\nplt.figure(figsize=(20, 15))\nplt.plot(argmaxx, \"*--\")\nplt.plot(argmaxx_per, \"*--\")\nplt.title(\"Argmax before - after \")\nplt.legend([\"Original\", \"Perturbed\"])\nplt.show()\n</code></pre> <pre><code># Compare softmax outputs for a single image (original and perturbed)\none_img = tf.expand_dims(img[17], axis=0)\none_img_perturbed = oodmodel._input_perturbation(one_img)\nprint(\"Diff\", np.abs((one_img - one_img_perturbed).numpy()).sum())\nprint()\n\nprint(\n    \"preds original\",\n    oodmodel.feature_extractor.model(one_img, training=False) / oodmodel.temperature,\n)\nprint(\n    \"argmax preds original\",\n    (oodmodel.feature_extractor.model(one_img, training=False) / oodmodel.temperature)\n    .numpy()\n    .argmax(),\n)\nprint()\nprint(\n    \"preds perturbed\",\n    oodmodel.feature_extractor.model(one_img_perturbed, training=False)\n    / oodmodel.temperature,\n)\nprint(\n    \"argmax preds perturbed\",\n    (\n        oodmodel.feature_extractor.model(one_img_perturbed, training=False)\n        / oodmodel.temperature\n    )\n    .numpy()\n    .argmax(),\n)\n</code></pre> <pre><code>print(oodmodel.feature_extractor.model.get_layer(\"conv2_block2_1_bn\").moving_mean.numpy()[:20])\n\nfor batch in ds_ood.take(1):\n    oodmodel.feature_extractor.model(100*batch)\n\n# print(oodmodel.feature_extractor.model.get_layer(\"conv2_block2_1_bn\").moving_mean.numpy()[:20])\n\n\nwith tf.GradientTape(watch_accessed_variables=True) as tape:\n    tape.watch(batch)\n    tape.watch(oodmodel.feature_extractor.model.get_layer(\"conv2_block2_1_bn\").variables)\n    preds = oodmodel.feature_extractor.model(batch, training=None)\n# grad = tape.gradient(preds, batch)\n_ = tape.gradient(preds, oodmodel.feature_extractor.model.get_layer(\"conv2_block2_1_bn\").variables)\n\n# print(grad.numpy().flatten()[:20])\nprint(oodmodel.feature_extractor.model.get_layer(\"conv2_block2_1_bn\").moving_mean.numpy()[:20])\n\n\n\n# print()\n# print(oodmodel.feature_extractor.model(batch[:3]))\n# print(oodmodel.feature_extractor.model(batch[:3], training=False))\n# print(oodmodel.feature_extractor.model(batch[:3], training=True))\n</code></pre>"},{"location":"notebooks/odin_benchmark/#benchmark-of-odin-method","title":"Benchmark of ODIN method","text":"<p>This notebook aims at evaluating the ODIN method and reproducing the results in the original paper.</p> <p>The authors compared two network architectures on two in-distribution datasets and six out-of-distribution datasets.</p> <ul> <li>Network architectures</li> <li>DenseNet-BC-100</li> <li>Wide-ResNet-28-10</li> <li>ID datasets</li> <li>CIFAR-10</li> <li>CIFAR-100</li> <li>OOD datasets</li> <li>TinyImageNet cropped</li> <li>TinyImageNet resized</li> <li>LSUN cropped</li> <li>LSUN resized</li> <li>Uniform noise</li> <li>Gaussian noise.</li> </ul> <p>Here, we focus on a DenseNet network trained on CIFAR-10. This model is challenged on LSUN and TinyImageNet cropped OOD datasets.</p> <p>Reference Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks Liang, Shiyu and Li, Yixuan and Srikant, R. International Conference on Learning Representations, 2018 https://openreview.net/forum?id=H1VGkIxRZ</p>"},{"location":"notebooks/odin_benchmark/#1-load-cifar-10-dataset-and-pretrained-densenet-model","title":"1. Load CIFAR-10 dataset and pretrained DenseNet model","text":"<p>The CIFAR-10 dataset is loaded and preprocessed (normalized to 0-1). This is our in-distribution dataset.</p> <p>A pretrained DenseNet-121 model is loaded and evaluated on CIFAR-10 test set: 87.4 % accuracy.</p>"},{"location":"notebooks/odin_benchmark/#2-load-lsun-cropped-dataset","title":"2. Load LSUN cropped dataset","text":"<p>Second, the LSUN cropped dataset is loaded and preprocessed. This is our out-of-distribution dataset.</p> <p>Note: see https://github.com/facebookresearch/odin to download OOD datasets from the original paper. Some OOD datasets have images of size 36x36 pixels: a black frame of 2 pixels surrounds the 32x32 image. In this case, the image is cropped to 32x32 pixels.</p>"},{"location":"notebooks/odin_benchmark/#3-ood-detection","title":"3. OOD detection","text":"<p>Here the ODIN method is applied on both ID and OOD images. The OOD score is computed for each image and some metrics are measured to evaluate the performance of the OOD detector.</p> <p>In a nutshell, the ODIN method consists of the three following steps:</p> <ol> <li>Perturb the input image by applying a gradient descent step in order to increase the    calibrated probability score of the predicted class.</li> <li>Compute the calibrated probability score of the perturbed image. This is defined as    the OOD score.</li> <li>If the OOD score is below a threshold, the image is considered as OOD.</li> </ol> <p>The calibrated probability is the temperature-scaled softmax of the logits where the temperature is a hyper-parameter of the method. The step in the gradient descent perturbation of the image is also a hyper-parameter.</p>"},{"location":"notebooks/odin_benchmark/#bonus","title":"BONUS","text":""}]}